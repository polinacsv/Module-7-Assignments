{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86c7c92-f5da-4a89-9aa7-4f3e06f85be7",
   "metadata": {
    "id": "d86c7c92-f5da-4a89-9aa7-4f3e06f85be7"
   },
   "source": [
    "# Homework 8: Predicting Power Loads with LSTM Networks\n",
    "\n",
    "## Due: Midnight on **Wednesday, October 29th** (with a 2-hour grace period)  \n",
    "**Worth:** 85 points\n",
    "\n",
    "In this assignment, you’ll build and analyze recurrent neural-network models to forecast hourly power demand, using the direct method, and comparing it with autoregressive methods.\n",
    "\n",
    "**Direct (Non-Autoregressive) Time Series Prediction:**\n",
    "\n",
    "![Screenshot 2025-10-19 at 11.17.16 AM.png](attachment:6c30ce0c-0c19-42ac-8a65-5623e080fad8.png)\n",
    "\n",
    "\n",
    "**What we'll do in this homework**\n",
    "\n",
    "Starting from a working LSTM baseline, you’ll experiment with how the **direct** forcasting models performs, and investigate how **lookback**, **forecast horizon**, and **architecture** affect predictive accuracy and generalization.  \n",
    "You’ll also explore an **autoregressive** variant, where the model rolls forward one hour at a time—an approach that often highlights how small errors can accumulate.\n",
    "\n",
    "You’ll complete three problems:\n",
    "\n",
    "* **Problem 1 — Exploring Lookback and Horizon:**  \n",
    "  Adjust how far back the model looks and how far ahead it predicts, and examine how these design choices influence MAE/RMSE.\n",
    "\n",
    "* **Problem 2 — Architectural Tweaks:**  \n",
    "  Modify the LSTM architecture by deepening the network, stacking layers, or adding a bidirectional layer, and evaluate the effect on validation performance.\n",
    "\n",
    "* **Problem 3 — Autoregressive Forecasting (ChatGPT Discussion):**  \n",
    "  Work interactively with ChatGPT to design a one-step-at-a-time autoregressive version of your model, and discuss how it compares to your direct multi-output model.\n",
    "\n",
    "\n",
    "\n",
    "For each problem, you'll report your best validation metrics, display the learning-curve plots, and briefly reflect on what you learned from the experiment.\n",
    "\n",
    "There are 5 graded problems, worth 17 points each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1bb1df-b23a-40c2-a1d8-44c4b44c2a34",
   "metadata": {
    "id": "db1bb1df-b23a-40c2-a1d8-44c4b44c2a34"
   },
   "source": [
    "## Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec81363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273c10a-aad2-43ab-8e21-749460bf3dc7",
   "metadata": {
    "id": "b273c10a-aad2-43ab-8e21-749460bf3dc7"
   },
   "outputs": [],
   "source": [
    "# ---------- Standard Library ----------\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Environment (set BEFORE importing TensorFlow)\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"        # Disable hash randomization\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"  # Deterministic TF ops where possible\n",
    "os.environ[\"TF_CUDNN_DETERMINISM\"] = \"1\"  # Deterministic CuDNN (if GPU)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress TF INFO/WARNING logs\n",
    "\n",
    "# ---------- Third-Party (General) ----------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,StratifiedShuffleSplit, ShuffleSplit\n",
    "import kagglehub\n",
    "\n",
    "# ---------- TensorFlow / Keras ----------\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers, initializers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay, ExponentialDecay\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, SpatialDropout1D, Bidirectional\n",
    "# Layers (vision)\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D,Conv1D, SeparableConv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "    GlobalMaxPooling2D, BatchNormalization, ReLU, Flatten, Dense, Dropout\n",
    ")\n",
    "\n",
    "# Preprocessing (vision)\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# ---------- (Optional) NLP-related imports ----------\n",
    "# If not doing NLP, you can safely remove this block.\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, GlobalAveragePooling1D, LSTM, GRU, SpatialDropout1D, Bidirectional, Lambda\n",
    ")\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.datasets import imdb  # uncomment only if needed\n",
    "\n",
    "# ---------- Reproducibility ----------\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.keras.utils.set_random_seed(random_seed)  # covers TF, Python, and NumPy RNGs\n",
    "\n",
    "# ---------- Convenience ----------\n",
    "he = initializers.HeNormal()\n",
    "l2reg = regularizers.l2(1e-4)\n",
    "\n",
    "def format_hms(seconds: float) -> str:\n",
    "    \"\"\"Format seconds as H:MM:SS.\"\"\"\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "# Example:\n",
    "# start = time.time()\n",
    "# ... your code ...\n",
    "# print(\"Elapsed:\", format_hms(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fcb85-cb8b-42e8-8bf9-7f6985d9dfcc",
   "metadata": {
    "id": "780fcb85-cb8b-42e8-8bf9-7f6985d9dfcc"
   },
   "source": [
    "### Utility function to plot learning curves and keep track of all results\n",
    "\n",
    "- Call `print_results()` to see listing of all results logged so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa5ec8-74b4-4e2c-bec4-67f6d112469e",
   "metadata": {
    "id": "e9aa5ec8-74b4-4e2c-bec4-67f6d112469e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Single global registry (scaled units)\n",
    "results = {}   # title -> dict with selected-epoch metrics and (optional) diagnostics\n",
    "\n",
    "def plot_learning_curves_scaled(\n",
    "    history,\n",
    "    title=\"Learning Curves — MAE & RMSE (scaled)\",\n",
    "    verbose=True,\n",
    "    record_globals=False,   # set True if you also want global minima as diagnostics\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot MAE/RMSE directly from Keras History (scaled units).\n",
    "    Select the epoch by min `val_loss` and record all metrics at that epoch.\n",
    "    \"\"\"\n",
    "    H = history.history if hasattr(history, \"history\") else history\n",
    "    if not isinstance(H, dict):\n",
    "        raise ValueError(\"history must be a Keras History or a dict\")\n",
    "\n",
    "    if \"loss\" not in H:\n",
    "        raise ValueError(\"History must contain 'loss' (MSE).\")\n",
    "\n",
    "    has_val_loss = \"val_loss\" in H and len(H[\"val_loss\"]) > 0\n",
    "\n",
    "    loss     = np.asarray(H[\"loss\"], dtype=float)\n",
    "    val_loss = np.asarray(H[\"val_loss\"], dtype=float) if has_val_loss else None\n",
    "\n",
    "    mae      = np.asarray(H[\"mae\"], dtype=float)      if \"mae\"      in H else None\n",
    "    val_mae  = np.asarray(H[\"val_mae\"], dtype=float)  if \"val_mae\"  in H else None\n",
    "    rmse     = np.asarray(H[\"rmse\"], dtype=float)     if \"rmse\"     in H else np.sqrt(loss)\n",
    "    val_rmse = np.asarray(H[\"val_rmse\"], dtype=float) if \"val_rmse\" in H else (np.sqrt(val_loss) if has_val_loss else None)\n",
    "\n",
    "    epochs = np.arange(1, len(loss) + 1)\n",
    "\n",
    "    # ----- Select epoch by min val_loss (or last if no validation) -----\n",
    "    if has_val_loss:\n",
    "        sel_idx = int(np.argmin(val_loss))\n",
    "        sel_note = \"min val_loss\"\n",
    "    else:\n",
    "        sel_idx = len(loss) - 1\n",
    "        sel_note = \"last epoch (no validation)\"\n",
    "\n",
    "    # Values at the selected epoch\n",
    "    sel = {\n",
    "        \"epoch\": sel_idx + 1,\n",
    "        \"selected_by\": sel_note,\n",
    "        \"loss\": float(loss[sel_idx]),\n",
    "        \"val_loss\": float(val_loss[sel_idx]) if has_val_loss else np.nan,\n",
    "        \"mae\": float(mae[sel_idx]) if mae is not None else np.nan,\n",
    "        \"val_mae\": float(val_mae[sel_idx]) if val_mae is not None and has_val_loss else np.nan,\n",
    "        \"rmse\": float(rmse[sel_idx]) if rmse is not None else (float(np.sqrt(loss[sel_idx])) if np.isfinite(loss[sel_idx]) else np.nan),\n",
    "        \"val_rmse\": (\n",
    "            float(val_rmse[sel_idx]) if (val_rmse is not None and has_val_loss)\n",
    "            else (float(np.sqrt(val_loss[sel_idx])) if has_val_loss and np.isfinite(val_loss[sel_idx]) else np.nan)\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Optional: global minima diagnostics\n",
    "    globs = {}\n",
    "    if record_globals:\n",
    "        if val_rmse is not None:\n",
    "            i = int(np.argmin(val_rmse))\n",
    "            globs.update(min_val_rmse=float(val_rmse[i]), epoch_min_val_rmse=i+1)\n",
    "        if val_mae is not None:\n",
    "            j = int(np.argmin(val_mae))\n",
    "            globs.update(min_val_mae=float(val_mae[j]), epoch_min_val_mae=j+1)\n",
    "\n",
    "    # ----- Plot (scaled units) -----\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    if rmse is not None:\n",
    "        ax.plot(epochs, rmse, label=\"train RMSE (scaled)\")\n",
    "    if val_rmse is not None:\n",
    "        ax.plot(epochs, val_rmse, label=\"val RMSE (scaled)\")\n",
    "    if mae is not None:\n",
    "        ax.plot(epochs, mae, linestyle=\"--\", label=\"train MAE (scaled)\")\n",
    "    if val_mae is not None:\n",
    "        ax.plot(epochs, val_mae, linestyle=\"--\", label=\"val MAE (scaled)\")\n",
    "\n",
    "    # Mark the selected epoch on validation curves\n",
    "    if has_val_loss and (val_rmse is not None or val_mae is not None):\n",
    "        if val_rmse is not None:\n",
    "            ax.scatter(sel[\"epoch\"], sel[\"val_rmse\"], marker=\"x\", s=70,\n",
    "                       label=f\"val RMSE @ {sel_note} = {sel['val_rmse']:.3g} (ep {sel['epoch']})\")\n",
    "        if val_mae is not None:\n",
    "            ax.scatter(sel[\"epoch\"], sel[\"val_mae\"], marker=\"o\", s=50,\n",
    "                       label=f\"val MAE @ {sel_note} = {sel['val_mae']:.3g} (ep {sel['epoch']})\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Error (scaled units)\")\n",
    "    ax.grid(True, linestyle=\":\", alpha=0.6)\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ----- Record (scaled) -----\n",
    "    final = {\n",
    "        \"selected_epoch\": sel[\"epoch\"],\n",
    "        \"selected_by\": sel[\"selected_by\"],\n",
    "        \"val_mae_at_min_valloss\": sel[\"val_mae\"],\n",
    "        \"val_rmse_at_min_valloss\": sel[\"val_rmse\"],\n",
    "        \"train_mae_at_sel\": sel[\"mae\"],\n",
    "        \"train_rmse_at_sel\": sel[\"rmse\"],\n",
    "        # keep last-epoch values if you want them too:\n",
    "        \"final_train_rmse\": float(rmse[-1])     if rmse is not None else np.nan,\n",
    "        \"final_val_rmse\":   float(val_rmse[-1]) if val_rmse is not None else np.nan,\n",
    "        \"final_train_mae\":  float(mae[-1])      if mae is not None else np.nan,\n",
    "        \"final_val_mae\":    float(val_mae[-1])  if val_mae is not None else np.nan,\n",
    "        \"units\":            \"scaled\",\n",
    "    }\n",
    "    final.update(globs)  # add diagnostics if requested\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Selected epoch: {final['selected_epoch']} ({final['selected_by']})\")\n",
    "        print(f\"Valid @ sel —  MAE: {final['val_mae_at_min_valloss']:.4g} | RMSE: {final['val_rmse_at_min_valloss']:.4g} (scaled)\")\n",
    "        print(f\"Train @ sel —  MAE: {final['train_mae_at_sel']:.4g}  | RMSE: {final['train_rmse_at_sel']:.4g} (scaled)\")\n",
    "\n",
    "    results[title] = final\n",
    "\n",
    "\n",
    "def print_results(sort_by=\"val_rmse_at_min_valloss\"):\n",
    "    if not results:\n",
    "        print(\"No results recorded yet.\"); return\n",
    "    ordering = sorted(results.items(), key=lambda kv: kv[1].get(sort_by, np.inf))\n",
    "    for name, info in ordering:\n",
    "        print(f\"{name:<35}  val_RMSE@min_val_loss={info.get('val_rmse_at_min_valloss', np.nan):.4g} \"\n",
    "              f\"| val_MAE@min_val_loss={info.get('val_mae_at_min_valloss', np.nan):.4g} \"\n",
    "              f\"| epoch={info.get('selected_epoch')}  (scaled)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d77645-942f-4e28-bc9b-f71520ea14ad",
   "metadata": {
    "id": "e7d77645-942f-4e28-bc9b-f71520ea14ad"
   },
   "source": [
    "###  Wrapper for training and testing\n",
    "\n",
    "#### Assumptions:   \n",
    "- Early stopping is default, add other callbacks as needed\n",
    "- Uses \"binary_crossentropy\" for binary classification task.\n",
    "- Assumes `X_train,y_train,X_val,y_val,X_test,y_test` already defined, accessed here as global variables, or can specify them when calling\n",
    "    - If you don’t pass X_train, it will look in the global namespace at call time.\n",
    "    - If you do pass an explicit X_train=..., that value takes priority.\n",
    "  \n",
    ">      # uses global X_train, y_train, ...\n",
    ">      train_and_test(model)\n",
    ">\n",
    ">      # uses explicitly passed datasets\n",
    ">      train_and_test(model, X_train=new_X, y_train=new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e0ee8-a3c6-43fe-a975-ecdead1b8da3",
   "metadata": {
    "id": "b92e0ee8-a3c6-43fe-a975-ecdead1b8da3"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_and_test(\n",
    "    model,\n",
    "    X_train=None, y_train=None,\n",
    "    X_val=None,   y_val=None,\n",
    "    X_test=None,  y_test=None,\n",
    "    *,\n",
    "    epochs=50,\n",
    "    optimizer=\"Adam\",\n",
    "    lr_schedule=1e-3,\n",
    "    clipnorm=1.0,\n",
    "    loss=\"mse\",                         # keep MSE for training\n",
    "    metrics=(\"mae\", \"rmse\"),            # track MAE & RMSE in curves\n",
    "    validation_split=0.2,\n",
    "    random_state=42,\n",
    "    title=\"Learning Curves (MSE loss; MAE & RMSE tracked)\",\n",
    "    batch_size=64,\n",
    "    use_early_stopping=True,\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "    callbacks=None,\n",
    "    verbose=0,\n",
    "    return_history=False,\n",
    "    plot_scaled_curves=True,            # uses plot_learning_curves_scaled if available\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains with MSE and tracks MAE/RMSE. Selects the epoch by min `val_loss`,\n",
    "    and reports *all* metrics at that same epoch to avoid cherry-picking.\n",
    "    Returns (history, test_results, info) if return_history=True else (test_results, info).\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Resolve datasets lazily from globals if not provided ----------\n",
    "    g = globals()\n",
    "    X_train = X_train if X_train is not None else g.get(\"X_train\")\n",
    "    y_train = y_train if y_train is not None else g.get(\"y_train\")\n",
    "    X_val   = X_val   if X_val   is not None else g.get(\"X_val\")\n",
    "    y_val   = y_val   if y_val   is not None else g.get(\"y_val\")\n",
    "    X_test  = X_test  if X_test  is not None else g.get(\"X_test\")\n",
    "    y_test  = y_test  if y_test  is not None else g.get(\"y_test\")\n",
    "\n",
    "    # ---------- Basic checks ----------\n",
    "    if X_train is None or y_train is None:\n",
    "        raise ValueError(\"Training data (X_train, y_train) not provided or defined globally.\")\n",
    "    if X_test is None or y_test is None:\n",
    "        raise ValueError(\"Test data (X_test, y_test) not provided or defined globally.\")\n",
    "\n",
    "    # ---------- If no explicit val provided, optionally create one from training ----------\n",
    "    made_internal_val = False\n",
    "    if X_val is None or y_val is None:\n",
    "        if validation_split and validation_split > 0.0:\n",
    "            X_tr = np.asarray(X_train)\n",
    "            y_tr = np.asarray(y_train)\n",
    "            test_size = float(validation_split)\n",
    "\n",
    "            def _is_single_label_classification(y):\n",
    "                try:\n",
    "                    y = np.asarray(y)\n",
    "                    return np.issubdtype(y.dtype, np.integer) and (len(np.unique(y)) < max(20, 0.02*len(y)))\n",
    "                except Exception:\n",
    "                    return False\n",
    "\n",
    "            if _is_single_label_classification(y_tr):\n",
    "                splitter = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "                (train_idx, val_idx), = splitter.split(X_tr, y_tr)\n",
    "            else:\n",
    "                splitter = ShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "                (train_idx, val_idx), = splitter.split(np.arange(len(X_tr)))\n",
    "\n",
    "            X_train, y_train = X_tr[train_idx], y_tr[train_idx]\n",
    "            X_val,   y_val   = X_tr[val_idx],   y_tr[val_idx]\n",
    "            made_internal_val = True\n",
    "        else:\n",
    "            if use_early_stopping:\n",
    "                raise ValueError(\"Early stopping needs validation. Provide (X_val, y_val) or set validation_split > 0.\")\n",
    "\n",
    "    if title:\n",
    "        print(f\"\\n{title}\\n\")\n",
    "\n",
    "    # ---------- Optimizer ----------\n",
    "    if isinstance(optimizer, str):\n",
    "        opt = getattr(tf.keras.optimizers, optimizer)(learning_rate=lr_schedule, clipnorm=clipnorm)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "\n",
    "    # ---------- Metrics ----------\n",
    "    keras_metrics = []\n",
    "    for m in metrics:\n",
    "        if isinstance(m, str):\n",
    "            key = m.lower()\n",
    "            if key in (\"mae\", \"meanabsoluteerror\"):\n",
    "                keras_metrics.append(tf.keras.metrics.MeanAbsoluteError(name=\"mae\"))\n",
    "            elif key in (\"rmse\", \"rootmeansquarederror\"):\n",
    "                keras_metrics.append(tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"))\n",
    "            elif key in (\"mse\", \"meansquarederror\"):\n",
    "                keras_metrics.append(tf.keras.metrics.MeanSquaredError(name=\"mse\"))\n",
    "            else:\n",
    "                keras_metrics.append(m)\n",
    "        else:\n",
    "            keras_metrics.append(m)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=keras_metrics)\n",
    "\n",
    "    # ---------- Callbacks ----------\n",
    "    cb = [] if callbacks is None else list(callbacks)\n",
    "    if use_early_stopping:\n",
    "        cb = [EarlyStopping(monitor=\"val_loss\", patience=patience, min_delta=min_delta,\n",
    "                            restore_best_weights=True, verbose=verbose)] + cb\n",
    "\n",
    "    # ---------- Fit ----------\n",
    "    start = time.time()\n",
    "    fit_kwargs = dict(\n",
    "        x=X_train, y=y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=cb,\n",
    "        verbose=verbose,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    if X_val is not None and y_val is not None:\n",
    "        fit_kwargs[\"validation_data\"] = (X_val, y_val)\n",
    "\n",
    "    history = model.fit(**fit_kwargs)\n",
    "    hist = history.history\n",
    "\n",
    "    # ---------- Select epoch by min val_loss ----------\n",
    "    if \"val_loss\" in hist and len(hist[\"val_loss\"]) > 0:\n",
    "        sel_idx = int(np.argmin(hist[\"val_loss\"]))       # 0-based\n",
    "        selected_by = \"val_loss\"\n",
    "    else:\n",
    "        # fallback: last epoch if no val set\n",
    "        sel_idx = len(hist.get(\"loss\", [])) - 1\n",
    "        selected_by = \"loss (no validation)\"\n",
    "\n",
    "    # Gather metrics at selected epoch (scaled units)\n",
    "    def _get(name):\n",
    "        v = hist.get(name, None)\n",
    "        return float(v[sel_idx]) if v is not None and len(v) > sel_idx else float(\"nan\")\n",
    "\n",
    "    selected = {\n",
    "        \"epoch\":         sel_idx + 1,\n",
    "        \"selected_by\":   selected_by,\n",
    "        \"loss\":          _get(\"loss\"),\n",
    "        \"val_loss\":      _get(\"val_loss\"),\n",
    "        \"mae\":           _get(\"mae\"),\n",
    "        \"val_mae\":       _get(\"val_mae\"),\n",
    "        \"rmse\":          _get(\"rmse\") if \"rmse\" in hist else float(np.sqrt(_get(\"loss\"))) if np.isfinite(_get(\"loss\")) else float(\"nan\"),\n",
    "        \"val_rmse\":      _get(\"val_rmse\") if \"val_rmse\" in hist else float(np.sqrt(_get(\"val_loss\"))) if np.isfinite(_get(\"val_loss\")) else float(\"nan\"),\n",
    "    }\n",
    "\n",
    "    # ---------- Evaluate on test set (scaled units) ----------\n",
    "    test_out = model.evaluate(X_test, y_test, verbose=0)\n",
    "    metric_names = model.metrics_names\n",
    "    test_results = dict(zip(metric_names, map(float, np.atleast_1d(test_out))))\n",
    "    if \"loss\" in test_results:\n",
    "        test_results[\"rmse_from_mse_loss\"] = float(np.sqrt(test_results[\"loss\"]))\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    # ---------- Console summary (report everything at selected epoch) ----------\n",
    "    # print(f\"Selected epoch: {selected['epoch']} (by {selected['selected_by']})\")\n",
    "    # print(f\"Train @ sel —  LOSS: {selected['loss']:.6f} | MAE: {selected['mae']:.6f} | RMSE: {selected['rmse']:.6f}\")\n",
    "    # if np.isfinite(selected[\"val_loss\"]):\n",
    "    #     print(f\"Valid @ sel —  LOSS: {selected['val_loss']:.6f} | MAE: {selected['val_mae']:.6f} | RMSE: {selected['val_rmse']:.6f}\")\n",
    "    # print(\"Test  (final weights after ES restore) — \" +\n",
    "    #       \" | \".join([f\"{k.upper()}: {v:.6f}\" for k, v in test_results.items()]) +\n",
    "    #       f\"   (time: {elapsed:.1f}s)\")\n",
    "\n",
    "    # ---------- Optional: plot scaled curves & record to registry ----------\n",
    "    if plot_scaled_curves and 'plot_learning_curves_scaled' in globals():\n",
    "        plot_learning_curves_scaled(history, title=title, verbose=verbose)\n",
    "\n",
    "    info = {\n",
    "        \"made_internal_val\": made_internal_val,\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"selected_epoch\": selected[\"epoch\"],\n",
    "        \"selected_by\": selected_by,\n",
    "        \"train_at_sel\": {\"loss\": selected[\"loss\"], \"mae\": selected[\"mae\"], \"rmse\": selected[\"rmse\"]},\n",
    "        \"val_at_sel\":   {\"loss\": selected[\"val_loss\"], \"mae\": selected[\"val_mae\"], \"rmse\": selected[\"val_rmse\"]},\n",
    "    }\n",
    "\n",
    "    if return_history:\n",
    "        return history, test_results, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e980c399-05a9-4a23-b768-d87c7a493389",
   "metadata": {
    "id": "e980c399-05a9-4a23-b768-d87c7a493389"
   },
   "source": [
    "### Learning Rate Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a019179-0a78-4d16-9773-30a91e0cc7ee",
   "metadata": {
    "id": "1a019179-0a78-4d16-9773-30a91e0cc7ee"
   },
   "outputs": [],
   "source": [
    "# Must be input as callback in train_and_test(....   , callbacks=[reduce_lr] )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',    # Quantity to be monitored.\n",
    "    factor=0.5,            # Factor by which the learning rate will be reduced.\n",
    "                           # new_lr = lr * factor\n",
    "    patience=5,            # Number of epochs with no improvement\n",
    "                           # after which learning rate will be reduced.\n",
    "    min_delta=1e-4,        # Threshold for measuring the new optimum,\n",
    "                           # to only focus on significant changes.\n",
    "    cooldown=0,            # Number of epochs to wait before resuming\n",
    "                           # normal operation after lr has been reduced.\n",
    "    min_lr=1e-8,           # Lower bound on the learning rate.\n",
    "    verbose=0,             # 0: quiet, 1: update messages.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff0434-0a03-4c39-80cf-c5b95619996a",
   "metadata": {
    "id": "b2ff0434-0a03-4c39-80cf-c5b95619996a"
   },
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WVmZ724nzph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5WVmZ724nzph",
    "outputId": "a8b0bc1f-a83e-4ec8-de57-c1d710dc8d81"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xK2F9JPboPfj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xK2F9JPboPfj",
    "outputId": "69d7f45f-1cba-4a0a-fdb9-b21b3306ebcf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gbR7oMz0o22g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbR7oMz0o22g",
    "outputId": "71f23768-3d09-473b-ac99-62c85f11c7a9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = '/content/drive/MyDrive'\n",
    "for filename in os.listdir(path):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeabbc1-6e40-4084-a527-2ccc330ed360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "efeabbc1-6e40-4084-a527-2ccc330ed360",
    "outputId": "c4c91afc-c54a-4656-d3fa-3a6bbd56d5fc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# --------------------------\n",
    "# 0) Load & reshape (Hr1..Hr24 only)\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/nyiso_loads.csv\")\n",
    "\n",
    "# strictly keep Hr1..Hr24 (drop Hr25 if present)\n",
    "hour_cols = sorted(\n",
    "    [c for c in df.columns if re.fullmatch(r\"Hr\\d+\", c) and 1 <= int(c[2:]) <= 24],\n",
    "    key=lambda c: int(c[2:])\n",
    ")\n",
    "\n",
    "# wide -> long (1 row per hour)\n",
    "long_df = df.melt(\n",
    "    id_vars=[\"Year\", \"Month\", \"Day\"],\n",
    "    value_vars=hour_cols,\n",
    "    var_name=\"Hour\",\n",
    "    value_name=\"Load\"\n",
    ").sort_values([\"Year\", \"Month\", \"Day\", \"Hour\"], ignore_index=True)\n",
    "\n",
    "# datetime index\n",
    "long_df[\"Datetime\"] = (\n",
    "    pd.to_datetime(long_df[[\"Year\", \"Month\", \"Day\"]]) +\n",
    "    pd.to_timedelta(long_df[\"Hour\"].str.extract(r\"(\\d+)\").astype(int)[0] - 1, unit=\"h\")\n",
    ")\n",
    "long_df = long_df.sort_values(\"Datetime\", ignore_index=True)\n",
    "\n",
    "# series\n",
    "load_series = pd.Series(long_df[\"Load\"].values, index=long_df[\"Datetime\"], name=\"NYISO_Load\")\n",
    "load_series.index.name = \"Datetime\"\n",
    "\n",
    "# --------------------------\n",
    "# 1) Keep ONLY the first six months from the series start\n",
    "# --------------------------\n",
    "start = load_series.index.min().normalize()\n",
    "end   = start + pd.DateOffset(months=5)   # exclusive upper bound\n",
    "s6    = load_series.loc[(load_series.index >= start) & (load_series.index < end)].copy()\n",
    "\n",
    "# Enforce hourly cadence & light gap fill\n",
    "s6 = s6.asfreq('h')             # pandas now prefers lowercase 'h'\n",
    "s6 = s6.interpolate(limit=3)    # fill gaps up to 3 hours\n",
    "s6 = s6.dropna()\n",
    "\n",
    "print(f\"Window kept: {s6.index[0]}  →  {s6.index[-1]}  ({len(s6)} hours)\")\n",
    "\n",
    "# --------------------------\n",
    "# 2) Chronological split: 70% / 15% / 15%\n",
    "# --------------------------\n",
    "N       = len(s6)\n",
    "n_train = int(0.70 * N)\n",
    "n_val   = int(0.15 * N)\n",
    "n_test  = N - n_train - n_val\n",
    "\n",
    "idx_tr = slice(0, n_train)\n",
    "idx_va = slice(n_train, n_train + n_val)\n",
    "idx_te = slice(n_train + n_val, N)\n",
    "\n",
    "t_train_end = s6.index[n_train]\n",
    "t_val_end   = s6.index[n_train + n_val]\n",
    "\n",
    "print(f\"Split sizes — train: {n_train}, val: {n_val}, test: {n_test}\")\n",
    "\n",
    "# --------------------------\n",
    "# 3) Plot Train / Val / Test\n",
    "# --------------------------\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.plot(s6.index[idx_tr], s6.values[idx_tr], color='black', label=f\"Train ({n_train} h)\")\n",
    "ax.plot(s6.index[idx_va], s6.values[idx_va], color='green', label=f\"Validation ({n_val} h)\")\n",
    "ax.plot(s6.index[idx_te], s6.values[idx_te], color='red',   label=f\"Test ({n_test} h)\")\n",
    "\n",
    "ax.axvline(t_train_end, linestyle='--', color='gray', alpha=0.7)\n",
    "ax.axvline(t_val_end,   linestyle='--', color='gray', alpha=0.7)\n",
    "\n",
    "ax.set_title(\"NYISO Load — First Six Months (Train / Val / Test)\")\n",
    "ax.set_xlabel(\"Datetime\")\n",
    "ax.set_ylabel(\"MWh\")\n",
    "ax.grid(True, axis='y', linestyle=':', alpha=0.6)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 4) Quick peek & NumPy export\n",
    "# --------------------------\n",
    "print(\"\\nMegaWatts per Hour — First Six Months\\n\")\n",
    "print(s6.head(6))\n",
    "print('....')\n",
    "print(s6.tail(6))\n",
    "print(f\"Length: {len(s6)} hours\")\n",
    "\n",
    "# For RNN/LSTM training\n",
    "load_array = s6.to_numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee9cc1-7eeb-414a-b1de-c9f8599d96de",
   "metadata": {
    "id": "37ee9cc1-7eeb-414a-b1de-c9f8599d96de"
   },
   "source": [
    "#### Plot Last Two Weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c25bcb-2b6a-445f-9607-2d32f70bdaa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "00c25bcb-2b6a-445f-9607-2d32f70bdaa6",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "370d29c5-a545-412a-95af-b2c211475efd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Choose the series to plot (use s6 if you limited to first six months)\n",
    "series = s6    # or load_series\n",
    "\n",
    "period_hours = 24 * 14\n",
    "sel = series.tail(min(period_hours, len(series)))  # last two weeks (or less if not available)\n",
    "vals = sel.values\n",
    "\n",
    "# positions of midnights within the selection\n",
    "midnight_mask = sel.index.hour == 0\n",
    "day_positions = np.nonzero(midnight_mask)[0]\n",
    "day_times = sel.index[midnight_mask]\n",
    "\n",
    "# weekday letter: Mon=M, Tue=T, Wed=W, Thu=Th, Fri=F, Sat/Sun=S\n",
    "def wd_letter(ts: pd.Timestamp) -> str:\n",
    "    dow = ts.dayofweek  # Mon=0 ... Sun=6\n",
    "    if dow >= 5:\n",
    "        return \"S\"\n",
    "    return [\"M\", \"T\", \"W\", \"Th\", \"F\"][dow]\n",
    "\n",
    "day_labels = [f\"{t:%m}:{t:%d} ({wd_letter(t)})\" for t in day_times]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.set_title(\"Last Two Weeks\")\n",
    "ax.plot(vals)\n",
    "ax.scatter(range(len(vals)), vals, marker='.', s=15)\n",
    "\n",
    "# vertical lines at each midnight\n",
    "for x in day_positions:\n",
    "    ax.axvline(x, linestyle='--', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax.set_xlim(0, len(vals) - 1)\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"MWh\")\n",
    "ax.grid(True, axis='y', linestyle=':', alpha=0.6)\n",
    "\n",
    "ax.set_xticks(day_positions)\n",
    "ax.set_xticklabels(day_labels, rotation=45, ha=\"right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef30a06-ec13-4723-b098-04cccfcb63ae",
   "metadata": {
    "id": "9ef30a06-ec13-4723-b098-04cccfcb63ae"
   },
   "source": [
    "### Preprocess Data into Sliding Windows\n",
    "\n",
    "\n",
    "The prediction task is to generate the next `horizon` predictions given the last `lookback` values from the time series data.  \n",
    "\n",
    "> Look at the diagram at the top of this notebook to see how lookback and horizon are used to create the training, validation, and testing sets.\n",
    "\n",
    "**Notes:**\n",
    "- The next cell uses 7*24 = 168 samples for lookback to capture weekly patterns in power usage; horizon is set to 1\n",
    "- This is **not autoregressive** as it does not include its predictions in its lookback.\n",
    "- Lookback for validation and testing sets will overlap with previous training and validation sets; this is not data leakage because only **past values** are used.\n",
    "- For `horizon > 1` you will get overlapping predictions and hence `horizon` predictions for almost all step; these are averaged to produce the actual predicted $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680d9e0-1c76-4413-a4f9-7bcf0c880edc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1680d9e0-1c76-4413-a4f9-7bcf0c880edc",
    "outputId": "996a3d30-7e01-4bc7-de12-d858afa22f77"
   },
   "outputs": [],
   "source": [
    "# assume s6 exists (your Jan–Jun hourly Series), and you computed n_train, n_val\n",
    "train = s6.iloc[:n_train]\n",
    "val   = s6.iloc[n_train:n_train+n_val]\n",
    "test  = s6.iloc[n_train+n_val:]\n",
    "\n",
    "# --------------------------\n",
    "# 3) Scaling (fit on train only)\n",
    "# --------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train.values.reshape(-1,1)).astype(np.float32)\n",
    "val_x   = scaler.transform(val.values.reshape(-1,1)).astype(np.float32)\n",
    "test_x  = scaler.transform(test.values.reshape(-1,1)).astype(np.float32)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Windowing utility\n",
    "# lookback = number of past hours used as features\n",
    "# horizon  = 1 (predict next hour)\n",
    "# --------------------------\n",
    "def make_windows(arr, lookback=168, horizon=1):\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(arr)-horizon+1):\n",
    "        X.append(arr[i-lookback:i, 0])\n",
    "        y.append(arr[i:i+horizon, 0])\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    # shapes: X -> (samples, lookback), y -> (samples, horizon)\n",
    "    return X[..., None], y  # add feature dim -> (samples, lookback, 1)\n",
    "\n",
    "lookback = 24 * 7   # use last 7 days of hourly load                           # <<<<<   here is where the lookback and horizon are set\n",
    "horizon  = 1        # next-hour forecast\n",
    "\n",
    "X_train, y_train = make_windows(train_x, lookback, horizon)\n",
    "X_val, y_val = make_windows(val_x,   lookback, horizon)\n",
    "X_test, y_test = make_windows(test_x,  lookback, horizon)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_test:\", y_test.shape)\n",
    "print(\"  X_val:\", X_val.shape, \"y_val:\", y_val.shape)\n",
    "print(\"  X_test:\", X_test.shape, \"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcc4f1-55dc-431a-9c7d-5c977526d798",
   "metadata": {
    "id": "42fcc4f1-55dc-431a-9c7d-5c977526d798"
   },
   "source": [
    "### Build a Baseline LSTM Model for lookback = 2 weeks, horizon = next hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840bc04e-cb7e-44c3-aeb6-a87f11bc2bc2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "840bc04e-cb7e-44c3-aeb6-a87f11bc2bc2",
    "outputId": "e9ed1871-a424-443b-ed22-ff620fa03155"
   },
   "outputs": [],
   "source": [
    "# Baseline LSTM model\n",
    "\n",
    "hn = keras.initializers.HeNormal(seed=random_seed)\n",
    "\n",
    "baseline_model = Sequential([\n",
    "    Input(shape=(lookback, 1)),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation=\"relu\", kernel_initializer=hn),\n",
    "    Dense(horizon)\n",
    "])\n",
    "\n",
    "\n",
    "train_and_test( baseline_model,title=\"Baseline Model\",verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9d87e-1162-4486-9845-f99f1eaeff1f",
   "metadata": {
    "id": "7fe9d87e-1162-4486-9845-f99f1eaeff1f"
   },
   "source": [
    "### Now we'll demonstrate the model's predictive power on the last 2 weeks of the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a3b94-c13e-491e-9673-f5ca60041c0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "ce1a3b94-c13e-491e-9673-f5ca60041c0e",
    "outputId": "a9789598-9dd8-4056-c771-42fcc7be6d11"
   },
   "outputs": [],
   "source": [
    "def evaluate_on_test(\n",
    "    model,\n",
    "    *,\n",
    "    target_scaler=None,          # pass your y scaler here if you want original units\n",
    "    report_in_scaled=True,       # set False to invert to original units\n",
    "    unit_scaled=\"(scaled)\",\n",
    "    unit_original=\"MW\",          # or \"MWh\" if your target is energy, not power\n",
    "    plot_last_hours=24*7,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate on X_test/y_test and plot the last `plot_last_hours` predictions.\n",
    "    Supports horizon=1 and direct multi-output (H>1).\n",
    "    \"\"\"\n",
    "    # 1) Predict\n",
    "    y_pred = model.predict(X_test, verbose=0)         # (n_windows, H) or (n_windows, 1)\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred[:, None]\n",
    "    H = y_pred.shape[1]\n",
    "\n",
    "    # 2) Choose units (scaled vs original)\n",
    "    def maybe_invert(arr):\n",
    "        if report_in_scaled or target_scaler is None:\n",
    "            return arr\n",
    "        a = np.asarray(arr).reshape(-1, 1)\n",
    "        return target_scaler.inverse_transform(a).ravel()\n",
    "\n",
    "    # 3) Build comparable arrays for metrics\n",
    "    # y_test matches y_pred shape: (n_windows, H)\n",
    "    y_true = y_test\n",
    "    # Metrics (per-horizon and averaged) in chosen units\n",
    "    err = maybe_invert(y_pred.ravel()) - maybe_invert(y_true.ravel())\n",
    "    rmse_avg = float(np.sqrt(np.mean(err**2)))\n",
    "    mae_avg  = float(np.mean(np.abs(err)))\n",
    "\n",
    "    if H > 1:\n",
    "        # per-horizon\n",
    "        y_true_u = maybe_invert(y_true)\n",
    "        y_pred_u = maybe_invert(y_pred)\n",
    "        mae_per_h  = np.mean(np.abs(y_true_u - y_pred_u), axis=0)\n",
    "        rmse_per_h = np.sqrt(np.mean((y_true_u - y_pred_u)**2, axis=0))\n",
    "\n",
    "    # 4) Prepare a single timeline for plotting\n",
    "    def stitch_direct_predictions(yhat):\n",
    "        \"\"\"Average overlaps for direct multi-step predictions.\"\"\"\n",
    "        n, HH = yhat.shape\n",
    "        total = n + HH - 1\n",
    "        out = np.zeros(total, dtype=float)\n",
    "        cnt = np.zeros(total, dtype=float)\n",
    "        for i in range(n):\n",
    "            out[i:i+HH] += yhat[i]\n",
    "            cnt[i:i+HH] += 1.0\n",
    "        return out / np.maximum(cnt, 1.0)\n",
    "\n",
    "    # Choose units for plotting\n",
    "    unit = unit_scaled if (report_in_scaled or target_scaler is None) else unit_original\n",
    "\n",
    "    if H == 1:\n",
    "        y_true_t = maybe_invert(y_true.ravel())\n",
    "        y_pred_t = maybe_invert(y_pred.ravel())\n",
    "    else:\n",
    "        # stitch both for a continuous timeline\n",
    "        y_pred_t = stitch_direct_predictions(maybe_invert(y_pred))\n",
    "        y_true_t = stitch_direct_predictions(maybe_invert(y_true))\n",
    "\n",
    "    # 5) Print summary\n",
    "    print(f\"Test RMSE (avg over all steps): {rmse_avg:,.3f} {unit}\")\n",
    "    print(f\"Test  MAE (avg over all steps): {mae_avg:,.3f} {unit}\")\n",
    "    if H > 1:\n",
    "        print(\"Per-horizon (0 = 1-step ahead):\")\n",
    "        print(\"  MAE :\", np.round(mae_per_h, 3))\n",
    "        print(\"  RMSE:\", np.round(rmse_per_h, 3))\n",
    "\n",
    "    # 6) Plot last N hours\n",
    "    N = min(plot_last_hours, len(y_true_t))\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.title(\"Test — Last 7 Days: Truth vs Forecast\")\n",
    "    plt.plot(y_true_t[-N:], label=\"Truth\")\n",
    "    plt.plot(y_pred_t[-N:], label=\"Predicted\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    plt.ylabel(unit)\n",
    "    plt.grid(True, axis='y', linestyle=':', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_on_test(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ae3f5-339f-4862-ba74-81712a0ddf98",
   "metadata": {
    "id": "e28ae3f5-339f-4862-ba74-81712a0ddf98"
   },
   "source": [
    "## Problem One — Exploring Lookback and Horizon\n",
    "\n",
    "Experiment with different combinations of `lookback` and `horizon` values.  \n",
    "For example, you might double the lookback window (e.g., from 1 week to 2 weeks) or extend the prediction horizon to 24 hours.  \n",
    "\n",
    "After each change:\n",
    "- Retrain your model.\n",
    "- Use `evaluate_on_test(...)` to visualize the extended 2-week prediction.\n",
    "- Compare validation and test MAE/RMSE across experiments.\n",
    "- Answer the graded questions.\n",
    "\n",
    "> *Note:* You’ll need to adjust the data-preparation cell above to rebuild the training, validation, and testing sets with the new parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae0edc5-0a3c-448d-89a0-3805e4ec85d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1ae0edc5-0a3c-448d-89a0-3805e4ec85d3",
    "outputId": "c92cd374-b5e5-4f85-c2a0-ed297afab63b"
   },
   "outputs": [],
   "source": [
    "# Your code here, add as many cells as you like\n",
    "\n",
    "\n",
    "def build_lstm(horizon, lookback, *, units=32, dropout=0.2, seed=42):\n",
    "    \"\"\"Baseline LSTM that outputs `horizon` steps directly (non-autoregressive).\"\"\"\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(lookback, 1)),\n",
    "        LSTM(units),\n",
    "        Dropout(dropout),\n",
    "        Dense(16, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(horizon)\n",
    "    ])\n",
    "\n",
    "def rebuild_windows(lookback, horizon):\n",
    "    \"\"\"Create windows for a given (lookback, horizon) using existing scaled splits.\"\"\"\n",
    "    X_tr, y_tr = make_windows(train_x, lookback, horizon)\n",
    "    X_va, y_va = make_windows(val_x,   lookback, horizon)\n",
    "    X_te, y_te = make_windows(test_x,  lookback, horizon)\n",
    "    return X_tr, y_tr, X_va, y_va, X_te, y_te\n",
    "\n",
    "def run_experiment(lookback, horizon, *, title=None, units=32, dropout=0.2, epochs=60, batch=64):\n",
    "    \"\"\"Train, plot curves, and visualize last-2-weeks forecast for one configuration.\"\"\"\n",
    "    if title is None:\n",
    "        title = f\"LB={lookback}h, H={horizon}h — LSTM({units})\"\n",
    "\n",
    "    # Window rebuild for this config\n",
    "    X_tr, y_tr, X_va, y_va, X_te, y_te = rebuild_windows(lookback, horizon)\n",
    "\n",
    "    # Expose to globals so train_and_test / evaluate_on_test can see them\n",
    "    globals().update(dict(\n",
    "        X_train=X_tr, y_train=y_tr,\n",
    "        X_val=X_va,   y_val=y_va,\n",
    "        X_test=X_te,  y_test=y_te\n",
    "    ))\n",
    "\n",
    "    model = build_lstm(horizon=horizon, lookback=lookback, units=units, dropout=dropout)\n",
    "\n",
    "    history, test_metrics, info = train_and_test(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch,\n",
    "        title=title,\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr],\n",
    "        return_history=True,\n",
    "        plot_scaled_curves=True,\n",
    "    )\n",
    "\n",
    "    # Visualize last two weeks\n",
    "    evaluate_on_test(model, plot_last_hours=24*14)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"selected_epoch\": info[\"selected_epoch\"],\n",
    "        \"val_rmse@min_valloss\": info[\"val_at_sel\"][\"rmse\"],\n",
    "        \"val_mae@min_valloss\": info[\"val_at_sel\"][\"mae\"],\n",
    "        \"test_rmse\": test_metrics.get(\"rmse_from_mse_loss\", float(\"nan\")),\n",
    "        \"test_mae\": test_metrics.get(\"mae\", float(\"nan\")),\n",
    "    }\n",
    "\n",
    "experiments = [\n",
    "    # Horizon = 1 (next hour), vary lookback\n",
    "    dict(lookback=24*3,   horizon=1,  units=32),  # 3 days\n",
    "    dict(lookback=24*7,   horizon=1,  units=32),  # 1 week (baseline)\n",
    "    dict(lookback=24*14,  horizon=1,  units=32),  # 2 weeks\n",
    "\n",
    "    # Horizon = 6 (next 6 hours direct)\n",
    "    dict(lookback=24*7,   horizon=6,  units=48),\n",
    "\n",
    "    # Horizon = 24 (next 24 hours direct)\n",
    "    dict(lookback=24*7,   horizon=24, units=64),\n",
    "\n",
    "    # Optional: longer memory for 24-h horizon\n",
    "    dict(lookback=24*14,  horizon=24, units=64),\n",
    "]\n",
    "\n",
    "summaries = []\n",
    "for cfg in experiments:\n",
    "    title = f\"LB={cfg['lookback']}h, H={cfg['horizon']}h — LSTM({cfg.get('units',32)})\"\n",
    "    out = run_experiment(**cfg, title=title, epochs=60, batch=64)\n",
    "    summaries.append(out)\n",
    "\n",
    "print(\"\\n=== Leaderboard (by val_RMSE@min_val_loss; scaled units) ===\")\n",
    "print_results(sort_by=\"val_rmse_at_min_valloss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8660458c-1e53-4394-9e09-5f9ede598f31",
   "metadata": {
    "id": "8660458c-1e53-4394-9e09-5f9ede598f31"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ctfc5L6A3jR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ctfc5L6A3jR",
    "outputId": "e4bad70c-10cd-4bb2-8866-3ce239c36edf"
   },
   "outputs": [],
   "source": [
    "print_results(sort_by=\"val_rmse_at_min_valloss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c2d56-fb0d-423d-8b87-1752d8a5687a",
   "metadata": {
    "id": "2c2c2d56-fb0d-423d-8b87-1752d8a5687a"
   },
   "outputs": [],
   "source": [
    "# Set a1 to the Validation MAE at the epoch of minimum validation loss (mse) for your best choices of lookback and horizon\n",
    "\n",
    "a1a = 0.08756             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886664e-a820-406a-807d-6a99a85f6402",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3886664e-a820-406a-807d-6a99a85f6402",
    "outputId": "1a88564f-f978-44de-9457-ee6a3f49a483"
   },
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a1a = {a1a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ec1ac-0961-4e5a-9246-65764884aab1",
   "metadata": {
    "id": "aa4ec1ac-0961-4e5a-9246-65764884aab1"
   },
   "source": [
    "#### Question a1b: Describe below your experiments. What effect did your changes to the lookback and horizon have? Did you get an improvement in the validation MAE?  \n",
    "\n",
    "#### Your Answer Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "haWVRSvYBH8d",
   "metadata": {
    "id": "haWVRSvYBH8d"
   },
   "source": [
    "Increasing the lookback from 3 days to 1 week helped since it captured weekly patterns, but 2 weeks didn’t add much and slightly hurt performance. Longer horizons (6–24 h) made errors grow, as expected. The best model was with a 72 h lookback and 1 h horizon, giving the lowest validation MAE (0.0876), a small improvement over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otW7S1IXBRhQ",
   "metadata": {
    "id": "otW7S1IXBRhQ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94956a40-0a29-46da-ac76-fbebedfc8bd4",
   "metadata": {
    "id": "94956a40-0a29-46da-ac76-fbebedfc8bd4"
   },
   "source": [
    "## Problem Two — Architectural Tweaks\n",
    "\n",
    "Using the best lookback and horizon from Problem 1 (or the original defaults if they performed better),  \n",
    "experiment with at least **one architectural change**.  \n",
    "\n",
    "Possible options:\n",
    "- Add a deeper “head” (e.g., extra Dense layers)\n",
    "- Stack another LSTM layer\n",
    "- Try a **Bidirectional LSTM** (it processes the past window in both forward and backward directions—no future leakage!)\n",
    "\n",
    "Train, evaluate, and answer the graded questions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd323be-4151-49ab-b07a-a152e5267f27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6dd323be-4151-49ab-b07a-a152e5267f27",
    "outputId": "e6bc2b55-427b-4a5e-a22e-e865aa8c411b"
   },
   "outputs": [],
   "source": [
    "# Your code here, add as many cells as you like\n",
    "LOOKBACK = 24 * 3   # 72 hours\n",
    "HORIZON  = 1        # 1-hour ahead\n",
    "\n",
    "X_train, y_train = make_windows(train_x, lookback=LOOKBACK, horizon=HORIZON)\n",
    "X_val,   y_val   = make_windows(val_x,   lookback=LOOKBACK, horizon=HORIZON)\n",
    "X_test,  y_test  = make_windows(test_x,  lookback=LOOKBACK, horizon=HORIZON)\n",
    "\n",
    "globals().update(dict(X_train=X_train, y_train=y_train,\n",
    "                      X_val=X_val,     y_val=y_val,\n",
    "                      X_test=X_test,   y_test=y_test))\n",
    "\n",
    "\n",
    "def baseline_lstm(units=32, dropout=0.2, seed=42):\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(LOOKBACK, 1)),\n",
    "        LSTM(units),\n",
    "        Dropout(dropout),\n",
    "        Dense(16, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(HORIZON)\n",
    "    ])\n",
    "\n",
    "def deeper_head(units=32, head_width=32, dropout=0.2, seed=42):\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(LOOKBACK, 1)),\n",
    "        LSTM(units),\n",
    "        Dropout(dropout),\n",
    "        Dense(head_width, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(16,        activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(HORIZON)\n",
    "    ])\n",
    "\n",
    "def stacked_lstm(units1=64, units2=32, dropout=0.2, seed=42):\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(LOOKBACK, 1)),\n",
    "        LSTM(units1, return_sequences=True),\n",
    "        LSTM(units2),\n",
    "        Dropout(dropout),\n",
    "        Dense(16, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(HORIZON)\n",
    "    ])\n",
    "\n",
    "def bidirectional_lstm(units=32, dropout=0.2, seed=42):\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(LOOKBACK, 1)),\n",
    "        Bidirectional(LSTM(units)),\n",
    "        Dropout(dropout),\n",
    "        Dense(16, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(HORIZON)\n",
    "    ])\n",
    "\n",
    "def run_architecture(name, build_fn, *, epochs=60, batch=64):\n",
    "    model = build_fn()\n",
    "    history, test_metrics, info = train_and_test(\n",
    "        model,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch,\n",
    "        title=f\"P2 — {name} (LB={LOOKBACK}h, H={HORIZON}h)\",\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr],\n",
    "        return_history=True,\n",
    "        plot_scaled_curves=True,\n",
    "    )\n",
    "    evaluate_on_test(model, plot_last_hours=24*14)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"selected_epoch\": info[\"selected_epoch\"],\n",
    "        \"val_rmse@min_valloss\": info[\"val_at_sel\"][\"rmse\"],\n",
    "        \"val_mae@min_valloss\": info[\"val_at_sel\"][\"mae\"],\n",
    "        \"test_rmse\": test_metrics.get(\"rmse_from_mse_loss\", float(\"nan\")),\n",
    "        \"test_mae\":  test_metrics.get(\"mae\", float(\"nan\")),\n",
    "    }\n",
    "\n",
    "arch_experiments = [\n",
    "    (\"Baseline LSTM\",           baseline_lstm),\n",
    "    (\"Deeper Head (Dense×2)\",   deeper_head),\n",
    "    (\"Stacked LSTM (64→32)\",    stacked_lstm),\n",
    "    (\"Bidirectional LSTM\",      bidirectional_lstm),\n",
    "]\n",
    "\n",
    "p2_summaries = []\n",
    "for name, fn in arch_experiments:\n",
    "    out = run_architecture(name, fn, epochs=60, batch=64)\n",
    "    p2_summaries.append(out)\n",
    "\n",
    "print(\"\\n=== Problem 2 Leaderboard (sorted by val_RMSE@min_val_loss; scaled) ===\")\n",
    "print_results(sort_by=\"val_rmse_at_min_valloss\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0545b-ae7e-4d71-ab17-baf250c5b1b3",
   "metadata": {
    "id": "41b0545b-ae7e-4d71-ab17-baf250c5b1b3"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q3q3OlE9B2bP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3q3OlE9B2bP",
    "outputId": "a8075c58-d7a9-4816-e7df-dd521c5caf62"
   },
   "outputs": [],
   "source": [
    "print_results(sort_by=\"val_rmse_at_min_valloss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d02bc-517f-4eff-92d5-57da042d6226",
   "metadata": {
    "id": "da1d02bc-517f-4eff-92d5-57da042d6226"
   },
   "outputs": [],
   "source": [
    "# Set a1 to the Validation MAE at the epoch of minimum validation loss (mse)\n",
    "\n",
    "a2a = 0.07201           # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579090f-cc5d-46ec-92fb-36b0936b6d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b579090f-cc5d-46ec-92fb-36b0936b6d38",
    "outputId": "50d78108-9836-4b37-e803-371dfe31e0ab"
   },
   "outputs": [],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a2a = {a2a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38951ae0-a230-4461-87bc-b10c77416bfe",
   "metadata": {
    "id": "38951ae0-a230-4461-87bc-b10c77416bfe"
   },
   "source": [
    "#### Question a2b: Describe below your experiments and what you observed. Did you get a better validation MAE through architectural changes?  \n",
    "\n",
    "#### Your Answer Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ScxTklicCGGo",
   "metadata": {
    "id": "ScxTklicCGGo"
   },
   "source": [
    "I tried a few tweaks using the 72-hour lookback and 1-hour horizon. Stacking LSTMs or adding more Dense layers didn’t help much, but the Bidirectional LSTM clearly did better, dropping the validation MAE to about 0.072. Letting the model read the window forward and backward seemed to help it catch short-term patterns more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8589c-6a30-4e12-916f-d87d5bab29db",
   "metadata": {
    "id": "00e8589c-6a30-4e12-916f-d87d5bab29db"
   },
   "source": [
    "## Problem Three — Autoregressive Forecasting (ChatGPT Discussion)\n",
    "\n",
    "In this final problem, you’ll explore **autoregressive forecasting**, where the model predicts one step ahead and feeds that prediction back as input to generate the next step.\n",
    "\n",
    "**Your task:**\n",
    "1. Briefly describe your current model to ChatGPT (e.g., “My LSTM predicts the next 24 hours directly from the past 168 hours.”), or share a short code snippet.  \n",
    "2. Ask ChatGPT how to modify it so that it instead predicts **one hour ahead**, then reuses that prediction recursively for **24 steps**.  \n",
    "3. Implement or outline the version it suggests.  \n",
    "4. Answer the graded question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17948a-2cf8-43e4-a7c5-d636bc116ee2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4c17948a-2cf8-43e4-a7c5-d636bc116ee2",
    "jp-MarkdownHeadingCollapsed": true,
    "outputId": "504f65a8-5225-4e9b-c533-f28c3b7e3f37"
   },
   "outputs": [],
   "source": [
    "# Your code here, add as many cells as you like\n",
    "# Goal: train a 1-step model (H=1) and roll it forward recursively for 24 steps.\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Bidirectional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Config: use your best LB from P1/P2; we'll do H=1 for training and roll 24 steps -----\n",
    "LOOKBACK = 24 * 3      # 72h (from P1 best)\n",
    "H_TRAIN  = 1           # one-step model\n",
    "H_ROLL   = 24          # roll forward 24 hours autoregressively\n",
    "\n",
    "# Rebuild seeds for 1-step training\n",
    "X_train, y_train = make_windows(train_x, lookback=LOOKBACK, horizon=H_TRAIN)\n",
    "X_val,   y_val   = make_windows(val_x,   lookback=LOOKBACK, horizon=H_TRAIN)\n",
    "X_test_1, y_test_1 = make_windows(test_x, lookback=LOOKBACK, horizon=H_TRAIN)\n",
    "\n",
    "# Also prepare multi-step ground truth for evaluation (same seeds, but H=24)\n",
    "X_test_seed, y_test_24 = make_windows(test_x, lookback=LOOKBACK, horizon=H_ROLL)\n",
    "\n",
    "# ----- 1-step model (use the winning BiLSTM head from P2) -----\n",
    "def one_step_bilstm(units=32, dropout=0.2, seed=42):\n",
    "    he = keras.initializers.HeNormal(seed=seed)\n",
    "    return Sequential([\n",
    "        Input(shape=(LOOKBACK, 1)),\n",
    "        Bidirectional(LSTM(units)),\n",
    "        Dropout(dropout),\n",
    "        Dense(16, activation=\"relu\", kernel_initializer=he),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "model_ar = one_step_bilstm()\n",
    "\n",
    "# Train with your existing utility (MSE loss; tracks MAE/RMSE in scaled units)\n",
    "globals().update(dict(X_train=X_train, y_train=y_train,\n",
    "                      X_val=X_val,     y_val=y_val,\n",
    "                      X_test=X_test_1, y_test=y_test_1))\n",
    "\n",
    "history, test_metrics, info = train_and_test(\n",
    "    model_ar,\n",
    "    epochs=60,\n",
    "    batch_size=64,\n",
    "    title=f\"P3 — 1-step BiLSTM (LB={LOOKBACK}h) for AR rolling\",\n",
    "    verbose=1,\n",
    "    callbacks=[reduce_lr],\n",
    "    return_history=True,\n",
    "    plot_scaled_curves=True,\n",
    ")\n",
    "\n",
    "# ----- Autoregressive roll-forward utility -----\n",
    "def autoregressive_predict(model, X_seed, steps: int):\n",
    "    \"\"\"\n",
    "    X_seed: (N, LOOKBACK, 1) seed windows (scaled).\n",
    "    steps : number of hours to roll forward.\n",
    "    Returns: (N, steps) predictions (scaled).\n",
    "    \"\"\"\n",
    "    N = X_seed.shape[0]\n",
    "    out = np.zeros((N, steps), dtype=np.float32)\n",
    "    # copy to avoid mutating caller\n",
    "    window = X_seed.copy()\n",
    "    for t in range(steps):\n",
    "        # predict next step for all seeds at once\n",
    "        yhat = model.predict(window, verbose=0).reshape(N)\n",
    "        out[:, t] = yhat\n",
    "        # slide window: drop first, append yhat\n",
    "        window = np.concatenate(\n",
    "            [window[:, 1:, :], yhat[:, None, None]],\n",
    "            axis=1\n",
    "        )\n",
    "    return out\n",
    "\n",
    "# Roll forward 24h for every seed in test\n",
    "y_pred_ar_24 = autoregressive_predict(model_ar, X_test_seed, steps=H_ROLL)   # (N, 24)\n",
    "\n",
    "# ----- Metrics (scaled units) -----\n",
    "def per_horizon_metrics(y_true, y_pred):\n",
    "    mae_per_h = np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "    rmse_per_h = np.sqrt(np.mean((y_true - y_pred)**2, axis=0))\n",
    "    mae_avg  = float(np.mean(np.abs(y_true - y_pred)))\n",
    "    rmse_avg = float(np.sqrt(np.mean((y_true - y_pred)**2)))\n",
    "    return mae_avg, rmse_avg, mae_per_h, rmse_per_h\n",
    "\n",
    "mae_avg, rmse_avg, mae_h, rmse_h = per_horizon_metrics(y_test_24, y_pred_ar_24)\n",
    "print(f\"[P3-AR] Test MAE (avg over 24):  {mae_avg:.4f} (scaled)\")\n",
    "print(f\"[P3-AR] Test RMSE (avg over 24): {rmse_avg:.4f} (scaled)\")\n",
    "print(\"Per-horizon MAE:\",  np.round(mae_h, 4))\n",
    "print(\"Per-horizon RMSE:\", np.round(rmse_h, 4))\n",
    "\n",
    "# ----- Quick plot: stitch the AR multi-step predictions for a continuous line -----\n",
    "def stitch_direct_predictions(yhat):\n",
    "    n, H = yhat.shape\n",
    "    total = n + H - 1\n",
    "    out = np.zeros(total, dtype=float); cnt = np.zeros(total, dtype=float)\n",
    "    for i in range(n):\n",
    "        out[i:i+H] += yhat[i]\n",
    "        cnt[i:i+H] += 1.0\n",
    "    return out / np.maximum(cnt, 1.0)\n",
    "\n",
    "y_pred_t = stitch_direct_predictions(y_pred_ar_24)\n",
    "y_true_t = stitch_direct_predictions(y_test_24)\n",
    "\n",
    "N = min(24*7, len(y_true_t))  # last 7 days\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.title(\"P3 — Autoregressive Roll (24h) — Test, Last 7 Days (scaled)\")\n",
    "plt.plot(y_true_t[-N:], label=\"Truth\")\n",
    "plt.plot(y_pred_t[-N:], label=\"AR Pred\", linestyle=\"--\")\n",
    "plt.xlabel(\"Hour\"); plt.ylabel(\"(scaled)\"); plt.grid(True, axis='y', linestyle=':', alpha=0.6)\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ef6dd-fc83-49e4-9b6c-adcbbc903cf6",
   "metadata": {
    "id": "b94ef6dd-fc83-49e4-9b6c-adcbbc903cf6"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Vha0HYqCbij",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Vha0HYqCbij",
    "outputId": "add42c02-b049-4296-8399-6522add67ac4"
   },
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0d259a-20de-447e-bdc6-b0dcc74b4826",
   "metadata": {
    "id": "dd0d259a-20de-447e-bdc6-b0dcc74b4826"
   },
   "source": [
    "#### Question a3: Answer the graded questions comparing the recursive and direct approaches:\n",
    "   - Does the error grow with horizon in the autoregressive version?\n",
    "   - Which approach you tried in this homework performs better overall?\n",
    "\n",
    "#### Your Answer Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MKRKh3PwCqBY",
   "metadata": {
    "id": "MKRKh3PwCqBY"
   },
   "source": [
    "Yes, in the autoregressive version the error definitely grew with each step, since each new prediction built on the last one. The early hours looked fine, but by 18–24 hours out the accuracy dropped. Overall, the direct Bidirectional LSTM from Problem 2 still performed better and stayed more stable than the recursive version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeee550b-e88d-49d0-b516-efa08cc18b33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeee550b-e88d-49d0-b516-efa08cc18b33",
    "outputId": "bae230ee-3407-489c-c601-ca8571a2b715"
   },
   "outputs": [],
   "source": [
    "print_results()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
