{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54d5436-a103-4743-8731-13e8510a2d94",
   "metadata": {
    "id": "c54d5436-a103-4743-8731-13e8510a2d94"
   },
   "source": [
    "## Homework 9: Text Classification with Fine-Tuned BERT\n",
    "\n",
    "### Due: Midnight on November 5th (with 2-hour grace period) — Worth 85 points\n",
    "\n",
    "In this final homework, we’ll explore **fine-tuning a pre-trained Transformer model (BERT)** for text classification using the **IMDB Movie Review** dataset. You’ll begin with a working baseline notebook and then conduct a series of controlled experiments to understand how data size, context length, and model architecture affect performance.\n",
    "\n",
    "You’ll complete three problems:\n",
    "\n",
    "* **Problem 1:** Evaluate how **sequence length** and **learning rate** jointly influence validation loss and generalization.\n",
    "* **Problem 2:** Measure how **training data size** affects both model performance and total training time.\n",
    "* **Problem 3:** Compare **two additional models** from the BERT family to analyze the trade-offs between model size and accuracy on this dataset.\n",
    "\n",
    "In each problem, you’ll report your key metrics, summarize what you observed, and reflect on what you learned.\n",
    "\n",
    "> **Note:** This homework was developed and tested on **Google Colab**, due to version conflicts when running locally. It is **strongly recommended** that you complete your work on Colab as well.\n",
    "\n",
    "There are 6 problems, each worth 14 points, and you get one point free if you complete the entire homework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "S-zVZFxlTWxP",
   "metadata": {
    "id": "S-zVZFxlTWxP"
   },
   "outputs": [],
   "source": [
    "# Install once per new Colab runtime\n",
    "%pip -q install -U keras keras-hub tensorflow tensorflow-text datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6035ef-078b-4ae3-8695-7e0d135cd51e",
   "metadata": {
    "id": "bb6035ef-078b-4ae3-8695-7e0d135cd51e"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras_hub as kh\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, Features, Value, ClassLabel\n",
    "\n",
    "from keras import mixed_precision                    # generally faster\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9307f-eb34-49e7-84e8-ed373dbd8ef3",
   "metadata": {
    "id": "50a9307f-eb34-49e7-84e8-ed373dbd8ef3"
   },
   "source": [
    "### Here is where you can set global hyperparameters for this homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189c9bbc-c634-417f-909b-ccc4d5945c76",
   "metadata": {
    "id": "189c9bbc-c634-417f-909b-ccc4d5945c76"
   },
   "outputs": [],
   "source": [
    "# ---------------- Config ----------------\n",
    "SEED        = 42\n",
    "MAX_LEN     = 128\n",
    "EPOCHS      = 3\n",
    "BATCH       = 32\n",
    "EVAL_BATCH  = 64\n",
    "SUBSET_FRAC = 0.25   # <-- 0.25 to train and test on 25% of whole dataset during development;  set to 1.0 for full dataset\n",
    "\n",
    "keras.utils.set_random_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d68ff-94e2-402a-8bd3-fef3cba9e7bf",
   "metadata": {
    "id": "d33d68ff-94e2-402a-8bd3-fef3cba9e7bf"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "### Load and Preprocess the IMDB Movie Review Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c3b6bd-ddc3-4c75-b0ba-8b09d2f42a96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "5c9e17610f334a4082887542a0d406ca",
      "e52a8e17a35f4c52afdf3200aa752fcc",
      "670bdc865f414c20ac7e64c1d995f126",
      "2204c2f168804b9b97ecf3ed8ea47de5",
      "d9c5872f80ad4eb390ebf243ed02979f",
      "428810987c8946ceacfea731891a6eac",
      "9122a4f3b2fa4626943bf6593dafa85b",
      "6e0c8fe34f12466283f68df51cc7836b",
      "ad8ca8a6d57f4e5d875df2210251121b",
      "da065289aaec4923b900ee1e5ee50ac6",
      "4214e307622143f8a2973bdaef851ff2",
      "869540a140dd4d20aae235709c2951a6",
      "6e76bb9032f7407ebb94668c4bf92e64",
      "5331802362a343ff972f2dfdfde8835b",
      "1d50757fa8d2477ab157876a55712430",
      "d1b3b79473344a9b9a8c41c9a38ea4d0",
      "9b45102f00194a429c405ffcf18bc346",
      "b001093087f54d17a03fa7b60b692036",
      "e7bfcb8c364a44f29b6409efd7d42d3b",
      "9f73e0edab7d4cd9973e123a0d3a8f04",
      "9f478a1554f046f99319f8abcfe2f1d9",
      "28cb6622407a48378e80ab8dc4caa180",
      "4b5833e2c52341bfaf52989c3196b282",
      "30710a95345f4d249214bfffbda4ef2e",
      "8e0a4b175c054b7998d065cb247c3194",
      "c2da14b5a74e4a5e83c2a03bb914a781",
      "bfe242c6c89549e88a5622ab6b7ad5fa",
      "7201dd9711254aff808bd92777ff8d07",
      "56654390b5c14a4583893f2cccb2dd4a",
      "59d9f76bae57408686dc23f00f526961",
      "7d132d0ef5ba4e18b741ebc0cc18da9f",
      "31fe90fc470c449da52adce062848d3e",
      "9cf749cf30e64786bd95f463e7e226fa",
      "a9a1932396a04550bdee3a94ca25f089",
      "ab4caec865f04774b9a81c180215e67d",
      "e42c57075766410d878af5ee39e5b376",
      "6ceae078c3df460998eae283d01efa7a",
      "bc766689c2254eedbc7cbcafab3095c7",
      "42d2447c7db548bbb936a1530e3d0865",
      "178b4e3b40a1497ba1170e6244bf6438",
      "619cc4774bd8407a9d68ebf342f4a5cc",
      "7d2bc1efe7224727a78e9c1de8455075",
      "5d89427641c44d5b9a1bf43da272ef1c",
      "029b061bfd0e4f849a4d2766c579d119",
      "14803c2ca47347fd83491463278e7ad1",
      "85d5c792ea6f4a9fb8314e6f956a573e",
      "debba33d11484c948887c7c1553f8655",
      "9cbce345a6894ee5836776d1e2c2170a",
      "52c2ed8cbfb948cfa23dafde192158a2",
      "7aaf35fa3d0a4dfd8782ac96646a046a",
      "5b2469d78c324f15b32f27cd3ed68d0b",
      "71f484a6e2004aa69a15fa8d0386a225",
      "a554fa7a0f7c403e9662f86bc5619db9",
      "e6cf2bf5e1aa4deeb33127b3800c1b5d",
      "e22461bc6a08499989d26218a0a6cceb",
      "f92c795bfe9d44dfa8f85e971623dcef",
      "82c9343aae8b48208ca4231e423b730d",
      "1b4b8f5e23fa48708a84edc8f4da6279",
      "f2a9efb9f58d4c549fff1ce3891baf26",
      "9d0e5b9a181646b387fc4ac79c84b912",
      "b41eeec0aa5d484c8a730a897fdc84f4",
      "cecdcb182c3c4283b63cf314eb4536f5",
      "0ce5f08d6ea745638195d56baedfd9a3",
      "9fefa1ff0b0243788038d09920b4292a",
      "4ff30ba04ae74af5b7b5c44803fc631f",
      "da89ed4f0d684fc296251cf579dae796",
      "0062408b2f704ef9989cb20669f552ed",
      "5e3ec3cd6b8c43dbaa281e0f8ebd8e6d",
      "b1300c27385f4fa59a4f26f0fb72758f",
      "540e2cee42df41e1af7336b35dfe2217",
      "6f8d08c73de348a295ec489c81d98204",
      "f8e93c037d444cfc963e330521de8005",
      "c5ae27e4bc254974a0b1ea9a2d81489b",
      "176ecc7f1dbc40958dba5bb2726fa80a",
      "73a4d73665e8454fb875c25f45289e04",
      "fd438debd8b34afebb19acc39c2e7cec",
      "34a9afd28993491abada8e87d8215a03"
     ]
    },
    "id": "59c3b6bd-ddc3-4c75-b0ba-8b09d2f42a96",
    "outputId": "8e8aace7-146f-4f42-b39e-f3533ac238e4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c9e17610f334a4082887542a0d406ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "869540a140dd4d20aae235709c2951a6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b5833e2c52341bfaf52989c3196b282"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9a1932396a04550bdee3a94ca25f089"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14803c2ca47347fd83491463278e7ad1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f92c795bfe9d44dfa8f85e971623dcef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0062408b2f704ef9989cb20669f552ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pool after SUBSET_FRAC=0.25: 12500 (of 50000)\n",
      "Train: (8750, [4375, 4375])  Val: (1250, [625, 625])  Test: (2500, [1250, 1250])\n"
     ]
    }
   ],
   "source": [
    "# ---- Load IMDb (raw), join train+test ----\n",
    "imdb   = load_dataset(\"imdb\")\n",
    "texts  = list(imdb[\"train\"][\"text\"]) + list(imdb[\"test\"][\"text\"])\n",
    "labels = np.array(list(imdb[\"train\"][\"label\"]) + list(imdb[\"test\"][\"label\"]), dtype=\"int32\")\n",
    "\n",
    "# ---- Build DS with explicit features (label=ClassLabel) ----\n",
    "features = Features({\"text\": Value(\"string\"),\n",
    "                     \"label\": ClassLabel(num_classes=2, names=[\"NEG\",\"POS\"])})\n",
    "all_ds = Dataset.from_dict({\"text\": texts, \"label\": labels.tolist()}, features=features)\n",
    "\n",
    "# ---- Optional: take a stratified subset of the FULL dataset ----\n",
    "if 0.0 < SUBSET_FRAC < 1.0:\n",
    "    sub = all_ds.train_test_split(train_size=SUBSET_FRAC, seed=SEED, stratify_by_column=\"label\")\n",
    "    ds_pool = sub[\"train\"]\n",
    "else:\n",
    "    ds_pool = all_ds\n",
    "\n",
    "# ---- Stratified 80/10/10 split on the (possibly smaller) pool ----\n",
    "# First: 80/20 train+val pool / test\n",
    "splits = ds_pool.train_test_split(test_size=0.20, seed=SEED, stratify_by_column=\"label\")\n",
    "train_val_pool, test_ds = splits[\"train\"], splits[\"test\"]\n",
    "# Then: carve 10% of full (i.e., 0.125 of the 80% pool) as validation\n",
    "splits2 = train_val_pool.train_test_split(test_size=0.125, seed=SEED, stratify_by_column=\"label\")\n",
    "train_ds, val_ds = splits2[\"train\"], splits2[\"test\"]\n",
    "\n",
    "# ---- Numpy arrays for Keras fit/predict ----\n",
    "X_tr = np.array(train_ds[\"text\"], dtype=object); y_tr = np.array(train_ds[\"label\"], dtype=\"int32\")\n",
    "X_va = np.array(val_ds[\"text\"],   dtype=object); y_va = np.array(val_ds[\"label\"],   dtype=\"int32\")\n",
    "X_te = np.array(test_ds[\"text\"],  dtype=object); y_te = np.array(test_ds[\"label\"],  dtype=\"int32\")\n",
    "\n",
    "# ---- Quick summary ----\n",
    "def _counts(ds):\n",
    "    arr = np.array(ds[\"label\"], dtype=int)\n",
    "    return len(arr), np.bincount(arr, minlength=2).tolist()\n",
    "print(f\"Pool after SUBSET_FRAC={SUBSET_FRAC}: {len(ds_pool)} (of {len(all_ds)})\")\n",
    "print(\"Train:\", _counts(train_ds), \" Val:\", _counts(val_ds), \" Test:\", _counts(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3cf44-614f-4b4b-801a-8865adf1ded3",
   "metadata": {
    "id": "c6e3cf44-614f-4b4b-801a-8865adf1ded3"
   },
   "source": [
    "### Build and train a baseline Distil-Bert Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4814b089-9299-4828-865a-81ae45bb2bb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "referenced_widgets": [
      "863e710e3e8040e4b6e5fc35a46f1db0",
      "d8428e14081d45749a92dbb745becf8d",
      "f1dc18cafec6427abd883d3eb6885c2f",
      "06c2ffe0ab53466b9bdb5da4a6067aa8",
      "8f9f82cc28cc4908899850e17091f3ff",
      "b615c75acdf7422fa1f466de007a3f72",
      "0aee3f569bcc4ef0b033af27531d56c2",
      "a3c24431898a4a188084dcb7b85b65a0",
      "29de9e932afa4d36ba54c7403c0cf736",
      "b1ab9701b79d46d68525c4dc8bc9ecd9",
      "aafed8f194994d729830ef144761f1c7",
      "1cd06e27dcf6469d80388c95a57e2860",
      "461e37b6fc3a4c3cbd1431f9552931bc",
      "163429447c384317a0daea79016d3bb7",
      "ca1708a4ecee42a7bce8daef57b2a36a",
      "bebc13a227d940cdbb3e3986f799c0a3",
      "5556030968a243b8a8f985bb14c31a49",
      "80a3afcabb6d433cbde1698dd9ed26e8",
      "721ea3c25e954b54a2e86b4f1471bc97",
      "49fd52be0d464191814dd7fb0cd49b28",
      "7b199f785fba428b82856a05c47eef42",
      "54386750ae994dfcac7bc1dafc4e0867"
     ]
    },
    "id": "4814b089-9299-4828-865a-81ae45bb2bb5",
    "outputId": "fec362cd-723a-47f3-d621-e27613521595"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 249ms/step - acc: 0.7824 - loss: 0.4530 - val_acc: 0.8376 - val_loss: 0.3447\n",
      "Epoch 2/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - acc: 0.8789 - loss: 0.2897 - val_acc: 0.8584 - val_loss: 0.3403\n",
      "Epoch 3/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - acc: 0.9159 - loss: 0.2207 - val_acc: 0.8600 - val_loss: 0.3555\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "863e710e3e8040e4b6e5fc35a46f1db0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd06e27dcf6469d80388c95a57e2860"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validation acc (best epoch): 0.858\n",
      "\n",
      "Test accuracy: 0.854   Test F1: 0.851\n",
      "\n",
      "Confusion matrix:\n",
      " [[1096  154]\n",
      " [ 210 1040]]\n",
      "\n",
      "Elapsed time: 00:02:48\n"
     ]
    }
   ],
   "source": [
    "# ---- Keras Hub preprocessor + classifier ----\n",
    "preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
    "    \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
    ")\n",
    "model = kh.models.DistilBertTextClassifier.from_preset(\n",
    "    \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# ---- Train with early stopping (restore best val weights) ----\n",
    "cb = [keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)]\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_va, y_va),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=cb,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# ---- Evaluate (accuracy + F1 via `evaluate`) ----\n",
    "logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
    "y_pred = logits.argmax(axis=-1)\n",
    "\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric  = evaluate.load(\"f1\")\n",
    "acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
    "f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
    "\n",
    "# Tiny confusion matrix helper (no sklearn needed)\n",
    "def confusion_matrix_np(y_true, y_pred, num_classes=2):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    return cm\n",
    "\n",
    "print(f\"\\nValidation acc (best epoch): {history.history['val_acc'][np.argmin(history.history['val_loss'])]:.3f}\")\n",
    "print(f\"\\nTest accuracy: {acc:.3f}   Test F1: {f1:.3f}\")\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix_np(y_te, y_pred))\n",
    "\n",
    "end = time.time() - start\n",
    "print(\"\\nElapsed time:\", time.strftime(\"%H:%M:%S\", time.gmtime(end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe4979-adac-4d73-aa57-a91cd0b2ad34",
   "metadata": {
    "id": "d2fe4979-adac-4d73-aa57-a91cd0b2ad34"
   },
   "source": [
    "# Problem 1 — Mini sweep: context length × learning rate (6 runs)\n",
    "\n",
    "In this problem we'll see how much **context length** (`MAX_LEN`) helps, and how sensitive fine-tuning is to **learning rate**—without running a huge grid.\n",
    "\n",
    "## Setup (keep these fixed)\n",
    "\n",
    "* `SUBSET_FRAC = 0.25`               # use only this percentage of the whole dataset\n",
    "* `EPOCHS = 3`\n",
    "* `BATCH = 32` (but see note for 256 below)\n",
    "* **EarlyStopping** with `restore_best_weights=True`\n",
    "* Same random `SEED` for all runs\n",
    "* Same data split for all runs (don’t reshuffle between runs)\n",
    "\n",
    "### Run these 6 configurations\n",
    "\n",
    "**For each** `MAX_LEN ∈ {128, 256, 512}`, try **two** learning rates:\n",
    "\n",
    "* **MAX_LEN = 128**\n",
    "\n",
    "  * `(LR = 2e-5, BATCH = 32)` – healthy default for shorter contexts.\n",
    "  * `(LR = 1e-5, BATCH = 32)` – conservative LR; often a touch stabler.\n",
    "\n",
    "* **MAX_LEN = 256**\n",
    "\n",
    "  * `(LR = 1e-5, BATCH = 16)` – longer context → lower batch.\n",
    "  * `(LR = 7.5e-6, BATCH = 16)` – even steadier if loss is noisy.\n",
    "\n",
    "* **MAX_LEN = 512**  *(heavier quadratic attention cost)*\n",
    "\n",
    "  * `(LR = 7.5e-6, BATCH = 8)` – safe starting point.\n",
    "  * `(LR = 5e-6, BATCH = 8)` – extra caution for stability.\n",
    "\n",
    "**If you hit an Out Of Memory error:**\n",
    "\n",
    "* At **256** with `BATCH = 16`, drop to `BATCH = 8`.\n",
    "* At **512** with `BATCH = 8`, drop to `BATCH = 4`.\n",
    "\n",
    "\n",
    "Then answer the graded questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd105932-3dc1-47e5-9b4a-90f4e9206143",
   "metadata": {
    "id": "bd105932-3dc1-47e5-9b4a-90f4e9206143",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3948beb7-b04c-4199-aa13-4805efce81f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Run: MAX_LEN=128 | LR=2e-05 | BATCH=32 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 159ms/step - acc: 0.8009 - loss: 0.4151 - val_acc: 0.8480 - val_loss: 0.3463\n",
      "Epoch 2/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - acc: 0.9024 - loss: 0.2484 - val_acc: 0.8360 - val_loss: 0.3904\n",
      "Epoch 3/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 34ms/step - acc: 0.9369 - loss: 0.1706 - val_acc: 0.8504 - val_loss: 0.3788\n",
      "→ val_acc@min_val_loss = 0.8480 | best_epoch=1 | time=106.5s\n",
      "\n",
      "=== Run: MAX_LEN=128 | LR=1e-05 | BATCH=32 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 160ms/step - acc: 0.7817 - loss: 0.4544 - val_acc: 0.8528 - val_loss: 0.3442\n",
      "Epoch 2/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - acc: 0.8802 - loss: 0.2905 - val_acc: 0.8568 - val_loss: 0.3484\n",
      "Epoch 3/3\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - acc: 0.9150 - loss: 0.2203 - val_acc: 0.8584 - val_loss: 0.3544\n",
      "→ val_acc@min_val_loss = 0.8528 | best_epoch=1 | time=108.5s\n",
      "\n",
      "=== Run: MAX_LEN=256 | LR=1e-05 | BATCH=16 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 136ms/step - acc: 0.8325 - loss: 0.3625 - val_acc: 0.9032 - val_loss: 0.2399\n",
      "Epoch 2/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 28ms/step - acc: 0.9186 - loss: 0.2072 - val_acc: 0.9064 - val_loss: 0.2463\n",
      "Epoch 3/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 28ms/step - acc: 0.9486 - loss: 0.1438 - val_acc: 0.9032 - val_loss: 0.2748\n",
      "→ val_acc@min_val_loss = 0.9032 | best_epoch=1 | time=165.0s\n",
      "\n",
      "=== Run: MAX_LEN=256 | LR=7.5e-06 | BATCH=16 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 91ms/step - acc: 0.8345 - loss: 0.3687 - val_acc: 0.9048 - val_loss: 0.2492\n",
      "Epoch 2/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 29ms/step - acc: 0.9135 - loss: 0.2203 - val_acc: 0.9032 - val_loss: 0.2492\n",
      "Epoch 3/3\n",
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 28ms/step - acc: 0.9386 - loss: 0.1643 - val_acc: 0.9040 - val_loss: 0.2664\n",
      "→ val_acc@min_val_loss = 0.9032 | best_epoch=2 | time=127.7s\n",
      "\n",
      "=== Run: MAX_LEN=512 | LR=7.5e-06 | BATCH=8 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 85ms/step - acc: 0.8647 - loss: 0.3130 - val_acc: 0.9136 - val_loss: 0.2181\n",
      "Epoch 2/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 31ms/step - acc: 0.9329 - loss: 0.1780 - val_acc: 0.9200 - val_loss: 0.2350\n",
      "Epoch 3/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 30ms/step - acc: 0.9611 - loss: 0.1113 - val_acc: 0.9152 - val_loss: 0.2452\n",
      "→ val_acc@min_val_loss = 0.9136 | best_epoch=1 | time=218.2s\n",
      "\n",
      "=== Run: MAX_LEN=512 | LR=5e-06 | BATCH=8 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 60ms/step - acc: 0.8451 - loss: 0.3435 - val_acc: 0.9176 - val_loss: 0.2232\n",
      "Epoch 2/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 30ms/step - acc: 0.9286 - loss: 0.1915 - val_acc: 0.9200 - val_loss: 0.2221\n",
      "Epoch 3/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 30ms/step - acc: 0.9515 - loss: 0.1350 - val_acc: 0.9152 - val_loss: 0.2255\n",
      "→ val_acc@min_val_loss = 0.9200 | best_epoch=2 | time=175.1s\n",
      "\n",
      "Summary (sorted by val_acc@min_val_loss desc):\n",
      "MAX_LEN=512  LR=5e-06     BATCH=8    val_acc@min=0.9200  best_ep=2   time=175.1s\n",
      "MAX_LEN=512  LR=7.5e-06   BATCH=8    val_acc@min=0.9136  best_ep=1   time=218.2s\n",
      "MAX_LEN=256  LR=1e-05     BATCH=16   val_acc@min=0.9032  best_ep=1   time=165.0s\n",
      "MAX_LEN=256  LR=7.5e-06   BATCH=16   val_acc@min=0.9032  best_ep=2   time=127.7s\n",
      "MAX_LEN=128  LR=1e-05     BATCH=32   val_acc@min=0.8528  best_ep=1   time=108.5s\n",
      "MAX_LEN=128  LR=2e-05     BATCH=32   val_acc@min=0.8480  best_ep=1   time=106.5s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9200000166893005"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Your code here; add as many cells as you need\n",
    "\n",
    "import time, numpy as np, keras, keras_hub as kh\n",
    "from keras import callbacks as kcb\n",
    "\n",
    "def best_val_acc_at_min_loss(history):\n",
    "    \"\"\"Return (val_acc_at_min_val_loss, best_epoch_index) from Keras History.\"\"\"\n",
    "    vl = history.history[\"val_loss\"]\n",
    "    va = history.history.get(\"val_acc\") or history.history.get(\"val_accuracy\")\n",
    "    i = int(np.argmin(vl))\n",
    "    return float(va[i]), i\n",
    "\n",
    "def run_one(MAX_LEN, LR, BATCH, epochs=EPOCHS):\n",
    "    preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
    "        \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
    "    )\n",
    "    model = kh.models.DistilBertTextClassifier.from_preset(\n",
    "        \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LR),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    es = kcb.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        hist = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=epochs,\n",
    "            batch_size=BATCH,\n",
    "            callbacks=[es],\n",
    "            verbose=1,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Friendly OOM fallback: try halving batch once\n",
    "        if \"OOM\" in str(e) or \"resource exhausted\" in str(e).lower():\n",
    "            alt_batch = max(1, BATCH // 2)\n",
    "            print(f\"[WARN] OOM at batch={BATCH}. Retrying with batch={alt_batch} …\")\n",
    "            hist = model.fit(\n",
    "                X_tr, y_tr,\n",
    "                validation_data=(X_va, y_va),\n",
    "                epochs=epochs,\n",
    "                batch_size=alt_batch,\n",
    "                callbacks=[es],\n",
    "                verbose=1,\n",
    "            )\n",
    "        else:\n",
    "            raise\n",
    "    wall = time.time() - start\n",
    "    val_acc_at_min, best_ep = best_val_acc_at_min_loss(hist)\n",
    "    return {\n",
    "        \"MAX_LEN\": MAX_LEN,\n",
    "        \"LR\": LR,\n",
    "        \"BATCH\": BATCH,\n",
    "        \"val_acc_at_min_val_loss\": val_acc_at_min,\n",
    "        \"best_epoch_idx\": best_ep,    # 0-based\n",
    "        \"epochs_trained\": len(hist.history[\"loss\"]),\n",
    "        \"wall_sec\": wall,\n",
    "    }\n",
    "\n",
    "sweep = [\n",
    "    # MAX_LEN=128\n",
    "    {\"MAX_LEN\":128, \"LR\":2e-5,  \"BATCH\":32},\n",
    "    {\"MAX_LEN\":128, \"LR\":1e-5,  \"BATCH\":32},\n",
    "    # MAX_LEN=256\n",
    "    {\"MAX_LEN\":256, \"LR\":1e-5,  \"BATCH\":16},\n",
    "    {\"MAX_LEN\":256, \"LR\":7.5e-6,\"BATCH\":16},\n",
    "    # MAX_LEN=512\n",
    "    {\"MAX_LEN\":512, \"LR\":7.5e-6,\"BATCH\":8},\n",
    "    {\"MAX_LEN\":512, \"LR\":5e-6,  \"BATCH\":8},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for cfg in sweep:\n",
    "    print(f\"\\n=== Run: MAX_LEN={cfg['MAX_LEN']} | LR={cfg['LR']} | BATCH={cfg['BATCH']} ===\")\n",
    "    out = run_one(**cfg, epochs=EPOCHS)\n",
    "    results.append(out)\n",
    "    print(\"→ val_acc@min_val_loss = {:.4f} | best_epoch={} | time={:,.1f}s\".format(\n",
    "        out[\"val_acc_at_min_val_loss\"], out[\"best_epoch_idx\"]+1, out[\"wall_sec\"]\n",
    "    ))\n",
    "\n",
    "# Pretty summary table\n",
    "print(\"\\nSummary (sorted by val_acc@min_val_loss desc):\")\n",
    "results_sorted = sorted(results, key=lambda r: r[\"val_acc_at_min_val_loss\"], reverse=True)\n",
    "for r in results_sorted:\n",
    "    print(\"MAX_LEN={:<3}  LR={:<8g}  BATCH={:<3}  val_acc@min={:.4f}  best_ep={:<2}  time={:,.1f}s\".format(\n",
    "        r[\"MAX_LEN\"], r[\"LR\"], r[\"BATCH\"], r[\"val_acc_at_min_val_loss\"], r[\"best_epoch_idx\"]+1, r[\"wall_sec\"]\n",
    "    ))\n",
    "\n",
    "# a1a = validation accuracy at min validation loss for the BEST configuration\n",
    "best = results_sorted[0]\n",
    "a1a = float(best[\"val_acc_at_min_val_loss\"])\n",
    "a1a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa6ea8a-6482-4c7b-81ba-aadbb3bfe564",
   "metadata": {
    "id": "1aa6ea8a-6482-4c7b-81ba-aadbb3bfe564"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a67d26c5-8294-438a-b29f-6a091180a1f8",
   "metadata": {
    "id": "a67d26c5-8294-438a-b29f-6a091180a1f8"
   },
   "outputs": [],
   "source": [
    "# Set a1a to the validation accuracy at min validation loss for your best configuration found in this problem\n",
    "\n",
    "a1a = 0.9200             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd1c33c-4761-4954-a04d-e4e4def945ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dd1c33c-4761-4954-a04d-e4e4def945ab",
    "outputId": "e6539536-843e-4478-b283-aad4b58de5e9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a1a = 0.9200\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a1a = {a1a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d2a89-301d-4f8d-9236-f88a1bc023ca",
   "metadata": {
    "id": "d46d2a89-301d-4f8d-9236-f88a1bc023ca"
   },
   "source": [
    "#### Question a1b:\n",
    "\n",
    "* Does **more context** (128 → 256 → 512) consistently help?\n",
    "* How much effect did the learning rate have on the validation accuracy?\n",
    "\n",
    "\n",
    "#### Your Answer Here:\n",
    "validation accuracy rose steadily from 85 % to 90 % to 92 % as the context length increased.\n",
    "The largest jump came between 128 and 256 tokens and 512 brought another small but clear gain, suggesting that longer context does capture more sentiment cues though with much higher compute cost."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The effect was modest but visible. Within each context length, the smaller learning rate was slightly more stable and gave 0.3–0.7 pp higher validation accuracy, especially for longer sequences where gradients can be noisier. At MAX_LEN = 128, 2e-5 and 1e-5 were nearly tied; for 256 and 512, the lower LR yielded the best convergence and avoided loss spikes."
   ],
   "metadata": {
    "id": "-NXVcUc7SV-b"
   },
   "id": "-NXVcUc7SV-b"
  },
  {
   "cell_type": "markdown",
   "id": "6ef49299-6f39-4b66-9e16-52417f3f917f",
   "metadata": {
    "id": "6ef49299-6f39-4b66-9e16-52417f3f917f"
   },
   "source": [
    "## Problem 2 — How much data is enough?\n",
    "\n",
    "In this problem, you’ll investigate how model performance scales with dataset size.\n",
    "\n",
    "**Setup.**\n",
    "Use the best `MAX_LEN` and `LR` values you found in **Problem 1**.\n",
    "\n",
    "**What to do:**\n",
    "\n",
    "1. For each value of `SUBSET_FRAC ∈ {0.25, 0.50, 0.75, 1.00}`, train your model once and observe the displayed performance metrics.\n",
    "2. Answer the discussion question below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82b9ebc5-809a-4e13-a4c6-1325b0d9a171",
   "metadata": {
    "id": "82b9ebc5-809a-4e13-a4c6-1325b0d9a171",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8b8e2cdc-0228-4bbb-aa81-a43cd7b79b4e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== Run: SUBSET_FRAC=0.25 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 60ms/step - acc: 0.8466 - loss: 0.3454 - val_acc: 0.9064 - val_loss: 0.2295\n",
      "Epoch 2/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 31ms/step - acc: 0.9275 - loss: 0.1948 - val_acc: 0.9152 - val_loss: 0.2322\n",
      "Epoch 3/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 30ms/step - acc: 0.9496 - loss: 0.1386 - val_acc: 0.9040 - val_loss: 0.2638\n",
      "→ val_acc@min=0.9064  test_acc=0.9144  f1=0.9146  time=202.6s\n",
      "\n",
      "=== Run: SUBSET_FRAC=0.5 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 50ms/step - acc: 0.8753 - loss: 0.2918 - val_acc: 0.9208 - val_loss: 0.1955\n",
      "Epoch 2/3\n",
      "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 30ms/step - acc: 0.9315 - loss: 0.1839 - val_acc: 0.9244 - val_loss: 0.1890\n",
      "Epoch 3/3\n",
      "\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 30ms/step - acc: 0.9514 - loss: 0.1365 - val_acc: 0.9216 - val_loss: 0.2206\n",
      "→ val_acc@min=0.9244  test_acc=0.9278  f1=0.9274  time=298.1s\n",
      "\n",
      "=== Run: SUBSET_FRAC=0.75 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 44ms/step - acc: 0.8885 - loss: 0.2699 - val_acc: 0.9256 - val_loss: 0.1979\n",
      "Epoch 2/3\n",
      "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 30ms/step - acc: 0.9350 - loss: 0.1754 - val_acc: 0.9267 - val_loss: 0.2054\n",
      "Epoch 3/3\n",
      "\u001b[1m3282/3282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 29ms/step - acc: 0.9548 - loss: 0.1284 - val_acc: 0.9269 - val_loss: 0.2163\n",
      "→ val_acc@min=0.9256  test_acc=0.9195  f1=0.9188  time=405.3s\n",
      "\n",
      "=== Run: SUBSET_FRAC=1.0 ===\n",
      "Epoch 1/3\n",
      "\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 30ms/step - acc: 0.8931 - loss: 0.2576 - val_acc: 0.9314 - val_loss: 0.1765\n",
      "Epoch 2/3\n",
      "\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 29ms/step - acc: 0.9371 - loss: 0.1679 - val_acc: 0.9334 - val_loss: 0.1861\n",
      "Epoch 3/3\n",
      "\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 29ms/step - acc: 0.9565 - loss: 0.1229 - val_acc: 0.9252 - val_loss: 0.2169\n",
      "→ val_acc@min=0.9314  test_acc=0.9237  f1=0.9244  time=454.7s\n",
      "\n",
      "Summary:\n",
      "SUBSET=0.25  n=12500   val@min=0.9064  test_acc=0.9144  f1=0.9146  time=202.6s\n",
      "SUBSET=0.50  n=25000   val@min=0.9244  test_acc=0.9278  f1=0.9274  time=298.1s\n",
      "SUBSET=0.75  n=37500   val@min=0.9256  test_acc=0.9195  f1=0.9188  time=405.3s\n",
      "SUBSET=1.00  n=50000   val@min=0.9314  test_acc=0.9237  f1=0.9244  time=454.7s\n"
     ]
    }
   ],
   "source": [
    "# Your code here; add as many cells as you need\n",
    "\n",
    "SEED = 42\n",
    "MAX_LEN = 512\n",
    "LR = 5e-6\n",
    "EPOCHS = 3\n",
    "BATCH = 8\n",
    "EVAL_BATCH = 64\n",
    "SUBSET_FRACS = [0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "def load_split(sub_frac):\n",
    "    imdb   = load_dataset(\"imdb\")\n",
    "    texts  = list(imdb[\"train\"][\"text\"]) + list(imdb[\"test\"][\"text\"])\n",
    "    labels = np.array(list(imdb[\"train\"][\"label\"]) + list(imdb[\"test\"][\"label\"]), dtype=\"int32\")\n",
    "\n",
    "    feats = Features({\"text\": Value(\"string\"),\n",
    "                      \"label\": ClassLabel(num_classes=2, names=[\"NEG\",\"POS\"])})\n",
    "    all_ds = Dataset.from_dict({\"text\": texts, \"label\": labels.tolist()}, features=feats)\n",
    "\n",
    "    ds_pool = (\n",
    "        all_ds.train_test_split(train_size=sub_frac, seed=SEED, stratify_by_column=\"label\")[\"train\"]\n",
    "        if 0.0 < sub_frac < 1.0 else all_ds\n",
    "    )\n",
    "\n",
    "    s1 = ds_pool.train_test_split(test_size=0.20, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_val_pool, test_ds = s1[\"train\"], s1[\"test\"]\n",
    "    s2 = train_val_pool.train_test_split(test_size=0.125, seed=SEED, stratify_by_column=\"label\")\n",
    "    train_ds, val_ds = s2[\"train\"], s2[\"test\"]\n",
    "\n",
    "    X_tr, y_tr = np.array(train_ds[\"text\"], dtype=object), np.array(train_ds[\"label\"], dtype=\"int32\")\n",
    "    X_va, y_va = np.array(val_ds[\"text\"], dtype=object), np.array(val_ds[\"label\"], dtype=\"int32\")\n",
    "    X_te, y_te = np.array(test_ds[\"text\"], dtype=object), np.array(test_ds[\"label\"], dtype=\"int32\")\n",
    "    return X_tr, y_tr, X_va, y_va, X_te, y_te, len(ds_pool)\n",
    "\n",
    "def run_one(sub_frac):\n",
    "    X_tr, y_tr, X_va, y_va, X_te, y_te, n = load_split(sub_frac)\n",
    "    preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\n",
    "        \"distil_bert_base_en_uncased\", sequence_length=MAX_LEN\n",
    "    )\n",
    "    model = kh.models.DistilBertTextClassifier.from_preset(\n",
    "        \"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(LR),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    es = kcb.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "    start = time.time()\n",
    "    hist = model.fit(X_tr, y_tr,\n",
    "                     validation_data=(X_va, y_va),\n",
    "                     epochs=EPOCHS, batch_size=BATCH,\n",
    "                     callbacks=[es], verbose=1)\n",
    "    val_acc = hist.history[\"val_acc\"][np.argmin(hist.history[\"val_loss\"])]\n",
    "    logits = model.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
    "    y_pred = logits.argmax(axis=-1)\n",
    "    acc_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric  = evaluate.load(\"f1\")\n",
    "    acc = acc_metric.compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
    "    f1  = f1_metric.compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
    "    dur = time.time() - start\n",
    "    return {\"SUBSET_FRAC\":sub_frac, \"n_samples\":n, \"val_acc@min\":val_acc,\n",
    "            \"test_acc\":acc, \"test_f1\":f1, \"time_sec\":dur}\n",
    "\n",
    "results2 = []\n",
    "for f in SUBSET_FRACS:\n",
    "    print(f\"\\n=== Run: SUBSET_FRAC={f} ===\")\n",
    "    out = run_one(f)\n",
    "    results2.append(out)\n",
    "    print(\"→ val_acc@min={:.4f}  test_acc={:.4f}  f1={:.4f}  time={:,.1f}s\".format(\n",
    "        out[\"val_acc@min\"], out[\"test_acc\"], out[\"test_f1\"], out[\"time_sec\"])\n",
    "    )\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for r in results2:\n",
    "    print(\"SUBSET={:<4.2f}  n={:<6}  val@min={:.4f}  test_acc={:.4f}  f1={:.4f}  time={:,.1f}s\".format(\n",
    "        r[\"SUBSET_FRAC\"], r[\"n_samples\"], r[\"val_acc@min\"], r[\"test_acc\"], r[\"test_f1\"], r[\"time_sec\"])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43868b2d-3a63-4422-a11c-b0eb7ada89a7",
   "metadata": {
    "id": "43868b2d-3a63-4422-a11c-b0eb7ada89a7"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00abc6af-69f5-494e-8349-9e0ebf115e42",
   "metadata": {
    "id": "00abc6af-69f5-494e-8349-9e0ebf115e42"
   },
   "outputs": [],
   "source": [
    "# Set a2a to the validation accuracy at min validation loss for your best configuration found in this problem\n",
    "# (Yes, it is probably at 1.0!)\n",
    "\n",
    "a2a = 0.9314            # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a4afa0b-5e7d-4ba1-8bb5-8d5229aef4fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4afa0b-5e7d-4ba1-8bb5-8d5229aef4fd",
    "outputId": "b6ef876f-b2ba-45a1-e5a1-377b9a99eb5d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a2a = 0.9314\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a2a = {a2a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba14dbe-08bd-4a03-ad90-2e2046a9ac78",
   "metadata": {
    "id": "5ba14dbe-08bd-4a03-ad90-2e2046a9ac78"
   },
   "source": [
    "#### Question a2b:\n",
    "\n",
    "Summarize what you observed as dataset size increased. Given that validation metrics are typically reliable to only about two decimal places, do the performance gains justify using the entire dataset? What trade-offs between accuracy and computation time did you notice?\n",
    "\n",
    "#### Your Answer Here:\n",
    "As I used more data, the accuracy went up a bit but started to level off after about 75%. The full dataset gave the best score (around 93%), but the gain over smaller subsets was tiny. Training time, though, almost doubled. So using all the data isn’t really worth it unless you need that last fraction of a percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c9212-dde0-4a33-b926-70ef9dddd1be",
   "metadata": {
    "id": "1e8c9212-dde0-4a33-b926-70ef9dddd1be"
   },
   "source": [
    "# Problem 3 — Model swap: speed vs. accuracy (why: capacity matters)\n",
    "\n",
    "In this problem we will compare encoder-only backbones of different sizes.\n",
    "\n",
    "**Setup.** Keep the best `MAX_LEN`, `LR`, and `SUBSET_FRAC` from Problems 1–2. Only change the model/preset:\n",
    "\n",
    "* **DistilBERT** (current baseline)\n",
    "* **BERT-base** (larger/usually stronger)\n",
    "\n",
    "**How to switch (two lines each).**\n",
    "\n",
    "* DistilBERT:\n",
    "\n",
    "  ```python\n",
    "  preproc = kh.models.DistilBertTextClassifierPreprocessor.from_preset(\"distil_bert_base_en_uncased\", sequence_length=MAX_LEN)\n",
    "  model  = kh.models.DistilBertTextClassifier.from_preset(\"distil_bert_base_en_uncased\", num_classes=2, preprocessor=preproc)\n",
    "  ```\n",
    "\n",
    "* BERT-base:\n",
    "\n",
    "  ```python\n",
    "  preproc = kh.models.BertTextClassifierPreprocessor.from_preset(\"bert_base_en_uncased\", sequence_length=MAX_LEN)\n",
    "  model  = kh.models.BertTextClassifier.from_preset(\"bert_base_en_uncased\", num_classes=2, preprocessor=preproc)\n",
    "  ```\n",
    "\n",
    "**What to do.**\n",
    "\n",
    "1. Train/evaluate each model once with identical settings.\n",
    "2. Observe the performance metrics for each.\n",
    "3. Answer the graded questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce5521c-e487-4ee6-8ce8-a93a5231eec9",
   "metadata": {
    "id": "0ce5521c-e487-4ee6-8ce8-a93a5231eec9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3c369eae-3e07-4f58-f59a-83bcc76b8b35"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "=== DistilBERT ===\n",
      "Epoch 1/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 61ms/step - acc: 0.8463 - loss: 0.3440 - val_acc: 0.9136 - val_loss: 0.2310\n",
      "Epoch 2/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 32ms/step - acc: 0.9274 - loss: 0.1953 - val_acc: 0.9104 - val_loss: 0.2276\n",
      "Epoch 3/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 31ms/step - acc: 0.9520 - loss: 0.1376 - val_acc: 0.9080 - val_loss: 0.2488\n",
      "→ val@min=0.9104  test_acc=0.9156  f1=0.9162  time=182.5s\n",
      "\n",
      "=== BERT-base ===\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/3/download/config.json...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 457/457 [00:00<00:00, 948kB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/3/download/tokenizer.json...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 761/761 [00:00<00:00, 1.43MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/3/download/assets/tokenizer/vocabulary.txt...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 226k/226k [00:00<00:00, 2.16MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_base_en_uncased/3/download/model.weights.h5...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 418M/418M [00:05<00:00, 84.5MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 106ms/step - acc: 0.8698 - loss: 0.3068 - val_acc: 0.9088 - val_loss: 0.2261\n",
      "Epoch 2/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 59ms/step - acc: 0.9419 - loss: 0.1654 - val_acc: 0.9144 - val_loss: 0.2255\n",
      "Epoch 3/3\n",
      "\u001b[1m1094/1094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 59ms/step - acc: 0.9694 - loss: 0.0970 - val_acc: 0.9048 - val_loss: 0.2707\n",
      "→ val@min=0.9144  test_acc=0.9152  f1=0.9174  time=324.9s\n",
      "\n",
      "Best model: BERT-base | a3a = 0.9143999814987183\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'DistilBERT': {'val_acc': 0.9103999733924866,\n",
       "  'test_acc': 0.9156,\n",
       "  'f1': 0.9161700437028208,\n",
       "  'time_sec': 182.5107421875},\n",
       " 'BERT-base': {'val_acc': 0.9143999814987183,\n",
       "  'test_acc': 0.9152,\n",
       "  'f1': 0.9174454828660437,\n",
       "  'time_sec': 324.9089639186859}}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Your code here; add as many cells as you wish\n",
    "\n",
    "\n",
    "import time, numpy as np, keras, keras_hub as kh, evaluate\n",
    "from keras import callbacks as kcb\n",
    "\n",
    "MAX_LEN = 512\n",
    "LR      = 5e-6\n",
    "EPOCHS  = 3\n",
    "BATCH   = 8\n",
    "EVAL_BATCH = 64\n",
    "\n",
    "def run_kh_model(kind, preset):\n",
    "    pre = kind[\"preproc\"].from_preset(preset, sequence_length=MAX_LEN)\n",
    "    mdl = kind[\"model\"].from_preset(preset, num_classes=2, preprocessor=pre)\n",
    "    mdl.compile(\n",
    "        optimizer=keras.optimizers.Adam(LR),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    "    )\n",
    "    es = kcb.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "    t0 = time.time()\n",
    "    hist = mdl.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                   epochs=EPOCHS, batch_size=BATCH, callbacks=[es], verbose=1)\n",
    "    wall = time.time() - t0\n",
    "\n",
    "    val_idx = int(np.argmin(hist.history[\"val_loss\"]))\n",
    "    val_acc = float(hist.history[\"val_acc\"][val_idx])\n",
    "\n",
    "    logits = mdl.predict(X_te, batch_size=EVAL_BATCH, verbose=0)\n",
    "    y_pred = logits.argmax(axis=-1)\n",
    "\n",
    "    acc = evaluate.load(\"accuracy\").compute(predictions=y_pred, references=y_te)[\"accuracy\"]\n",
    "    f1  = evaluate.load(\"f1\").compute(predictions=y_pred, references=y_te)[\"f1\"]\n",
    "    return val_acc, acc, f1, wall\n",
    "\n",
    "kinds = {\n",
    "    \"DistilBERT\": {\n",
    "        \"preproc\": kh.models.DistilBertTextClassifierPreprocessor,\n",
    "        \"model\":   kh.models.DistilBertTextClassifier,\n",
    "        \"preset\":  \"distil_bert_base_en_uncased\",\n",
    "    },\n",
    "    \"BERT-base\": {\n",
    "        \"preproc\": kh.models.BertTextClassifierPreprocessor,\n",
    "        \"model\":   kh.models.BertTextClassifier,\n",
    "        \"preset\":  \"bert_base_en_uncased\",\n",
    "    }\n",
    "}\n",
    "\n",
    "results3 = {}\n",
    "for name, spec in kinds.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    vacc, tacc, f1, secs = run_kh_model(\n",
    "        {\"preproc\": spec[\"preproc\"], \"model\": spec[\"model\"]}, spec[\"preset\"]\n",
    "    )\n",
    "    results3[name] = dict(val_acc=vacc, test_acc=tacc, f1=f1, time_sec=secs)\n",
    "    print(f\"→ val@min={vacc:.4f}  test_acc={tacc:.4f}  f1={f1:.4f}  time={secs:,.1f}s\")\n",
    "\n",
    "# Set a3a = best validation accuracy at min val loss\n",
    "best_name = max(results3, key=lambda k: results3[k][\"val_acc\"])\n",
    "a3a = float(results3[best_name][\"val_acc\"])\n",
    "print(\"\\nBest model:\", best_name, \"| a3a =\", a3a)\n",
    "results3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f3698-11a9-4451-a53e-a2e92e18421a",
   "metadata": {
    "id": "147f3698-11a9-4451-a53e-a2e92e18421a"
   },
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d7925a-2494-4689-b329-0b625827a0a8",
   "metadata": {
    "id": "18d7925a-2494-4689-b329-0b625827a0a8"
   },
   "outputs": [],
   "source": [
    "# Set a1a to the validation accuracy at min validation loss for your best model found in this problem\n",
    "\n",
    "a3a = 0.9144           # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deec774a-2360-494d-9e2f-989164095a79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deec774a-2360-494d-9e2f-989164095a79",
    "outputId": "3c327bf0-5742-4e3b-f704-b10294b5f52d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a3a = 0.9144\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way\n",
    "\n",
    "print(f'a3a = {a3a:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6ce4d-28a7-4414-8d1c-114a6bc5fa5f",
   "metadata": {
    "id": "58b6ce4d-28a7-4414-8d1c-114a6bc5fa5f"
   },
   "source": [
    "#### Question a3b:\n",
    "\n",
    "**Answer briefly.**\n",
    "\n",
    "* Which model gives the best **accuracy/F1**?\n",
    "* Which is **fastest** per epoch?\n",
    "* Given limited development time or compute resources, which model is the best **overall choice** and why?\n",
    "\n",
    "#### Your Answer Here:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "BERT-base gave slightly better accuracy and F1 than DistilBERT..\n",
    "DistilBERT was much faster—about 180 s vs. 325 s per run.\n",
    "If time or compute is limited, DistilBERT is the better all-around choice since it’s quicker and nearly as accurate."
   ],
   "metadata": {
    "id": "o7OsqkBrgker"
   },
   "id": "o7OsqkBrgker"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}